---
layout: documents
title: "MLOps On top of Service Foundry"
author: Young Gyu Kim
email: credemol@gmail.com
summary: ""
tags: ""
breadcrumb:
  - name: Home
    url: /
  - name: Docs
    url: /documents/
  - name: MLOps Foundry
    url: /documents/mlops-foundry/

---

= MLOps On top of Service Foundry

:imagesdir: images/

.MLOps on top of Service Foundry
[.img-wide]
image::service-foundry-mlops-intro.png[align=center]

== Introduction

Machine Learning Operations (MLOps) is the practice of delivering machine learning models to production securely, reliably, and consistently. However, setting up a robust MLOps platform typically requires integrating a complex web of open-source tools, managing intricate infrastructure, and developing custom automation.

Service Foundry accelerates this journey by providing a production-ready foundation for MLOps. It eliminates the complexity of assembling tools manually by offering a pre-configured, GitOps-driven platform. With Service Foundry, data science and engineering teams can quickly spin up essential ML workloads using enterprise-grade open-source components.

== MLOps Simplified with Service Foundry

Service Foundry is a curated stack of open-source tools combined with a UI-driven GitOps console. It provides a seamless experience for deploying and managing the entire machine learning lifecycle—from data processing and model development to deployment and monitoring.

By leveraging the fundamental components of Service Foundry, you can easily spin up ML workloads:

* *Declarative Deployments:* Use standardized GitOps patterns (via ArgoCD) to manage your ML infrastructure and models declaratively.
* *Self-Service Workloads:* Data scientists can provision their own ML environments without waiting on infrastructure operations teams.
* *Integrated Ecosystem:* Core platform services like ingress routing (Traefik), certificate management (cert-manager), identity management (Keycloak), and secure secrets (Sealed Secrets) are provided out-of-the-box, allowing you to focus entirely on your ML pipelines rather than infrastructure plumbing.

== The MLOps Ecosystem

Service Foundry seamlessly integrates the industry-standard open-source MLOps ecosystem. Instead of spending weeks configuring these tools to work together, Service Foundry allows you to deploy them in minutes:

=== 1. Model Development
* *JupyterHub:* A multi-user environment for data scientists to explore data, develop models, and collaborate using Jupyter Notebooks. Hosted directly on your cluster, it provides secure, scalable compute resources for data exploration and experimentation.

=== 2. Experiment Tracking and Model Registry
* *MLflow:* The central hub for tracking experiments, packaging code into reproducible runs, and sharing and deploying models. MLflow integrates natively with Cloud Storage for artifact management and PostgreSQL for tracking data, all of which are seamlessly provisioned by Service Foundry.

=== 3. Feature Store
* *Feast Feature Server:* An operational data system for managing and serving machine learning features. It ensures consistency between training and serving data, allowing ML models to reliably consume features in production.

=== 4. Model Serving
* *KServe:* A highly scalable and flexible model serving platform on Kubernetes. It abstracts away the complexity of autoscaling, networking, and health checking, allowing you to easily deploy trained models (from MLflow or Cloud Storage) as production-ready REST/gRPC APIs.

=== 5. Data Processing and Pipelines
* *Apache Spark:* For large-scale distributed data processing, data transformation, and complex feature engineering.
* *Apache Airflow:* For orchestrating complex data pipelines, scheduling ML training jobs, and automating the end-to-end MLOps workflow.
* *Apache Kafka:* For high-throughput, low-latency event streaming and real-time data ingestion.
* *dbt (data build tool):* For transforming, testing, and documenting data in the data warehouse, enabling analytics engineering best practices.

=== 6. Storage and Databases
* *Cloud Storage (S3/MinIO):* Reliable object storage for datasets, MLflow tracking artifacts, and serialized models.
* *Neo4j:* High-performance graph database optimized for connected data, enabling advanced feature engineering for graph-based models.
* *PostgreSQL & Redis:* Fundamental data stores for platform metadata, application state, and fast-access feature serving interfaces.

== How Service Foundry Accelerates MLOps

### Spin up Workloads in Minutes

Traditionally, setting up MLflow, KServe, and JupyterHub on a Kubernetes cluster with secure endpoints, authentication, and persistent storage requires deep infrastructure expertise and significant time.

Service Foundry abstracts this complexity. Through its GitOps workflows, spinning up an ML ecosystem is as simple as defining your desired state in Git. The platform engine automatically provisions the resources, configures the networking routes, and wires up the dependent services.

### Security and Observability Built-In

Machine learning models require the same level of security and monitoring as traditional software systems:

* *Authentication:* Secure your JupyterHub, Airflow, and MLflow dashboards using Keycloak Single Sign-On (SSO).
* *Encrypted Secrets:* Manage access keys for Cloud Storage, model registries, and databases securely using Sealed Secrets.
* *Monitoring:* Track model serving performance metrics (such as latency and throughput) out-of-the-box using the integrated observability stack (OpenTelemetry, Prometheus, Grafana).

== Community Edition - Free and Full-Featured

Service Foundry Community Edition is a free, open-source version that provides full access to all platform features to build your MLOps stack, with no proprietary subscriptions required.

*Get Started:*

* Download the Community Edition License and Installation scripts at https://community.servicefoundry.org
* Watch the Installation video: https://youtu.be/kvDYjjZIeUQ
* Explore technical tutorials on our YouTube channel: https://www.youtube.com/@servicefoundry

== What You Need to Run Service Foundry

Service Foundry has minimal requirements and can be deployed in just 10 minutes on AWS:

=== Infrastructure Requirements
* *A Kubernetes Cluster:* AWS Elastic Kubernetes Service (EKS) (v1.33 or higher).
* *A Git Repository:* For storing GitOps configurations (GitHub, GitLab, Bitbucket).

=== Bootstrap Process
The Service Foundry Bootstrap tool automates the installation of ArgoCD and core GitOps components. Once bootstrapped, you can seamlessly deploy the MLOps ecosystem—like MLflow, KServe, Feast, and JupyterHub—directly from your version-controlled repository.

== Getting Started

Ready to deploy your MLOps platform on AWS EKS? 

=== Step 1: Obtain the Community Edition License
1. Visit the Service Foundry Community portal: https://community.servicefoundry.org/
2. Complete the registration to download your license file.

.Download the Community Edition License
[.img-wide]
image::community-edition-license.png[Community Edition License Download Page]

=== Step 2: Download the Bootstrap Package
1. Navigate to *Bootstrap Scripts* on the community portal.
2. Download and extract the ZIP file containing installation scripts, AWS IAM policies, and configuration templates.

.Download the Bootstrap Scripts
[.img-wide]
image::service-foundry-bootstrap.png[width=800, height=600]

=== Step 3: Prepare Your Environment
Ensure you have the AWS CLI, `kubectl`, `helm`, and `git` installed. AWS CloudShell is recommended for a quick, pre-authenticated setup.

For detailed installation instructions, refer to the link:installation.adoc[Installation Guide].

== Conclusion

By building your MLOps capability on top of Service Foundry, you bridge the gap between data science and IT operations. You empower your ML teams with the tools they need—MLflow, KServe, Feast, JupyterHub, and Airflow—without the burden of managing the underlying Kubernetes infrastructure, networking, and security.

*Ready to accelerate your MLOps journey?* Visit https://community.servicefoundry.org to download your free Community Edition license and spin up your ML workloads today.
