= Using Longhorn - Snapshots, S3 Backups, and Restore Operations

:imagesdir: images

[.img-wide]
image::longhorn-snapshots-and-backups.png[]

== Introduction

Longhorn is a cloud-native, distributed block storage system designed specifically for Kubernetes environments. As a CNCF (Cloud Native Computing Foundation) project, it provides enterprise-grade persistent storage capabilities with built-in high availability and comprehensive disaster recovery features. Unlike traditional storage solutions, Longhorn is designed from the ground up to work seamlessly within containerized environments, providing reliable block storage that can be dynamically attached to Kubernetes pods.

At its core, Longhorn transforms the local storage available on your Kubernetes nodes into a distributed storage system. It creates multiple replicas of each volume across different nodes, ensuring your data remains available even when individual nodes or disks fail. However, high availability alone is not sufficient for production environments—you also need protection against data corruption, accidental deletion, and cluster-wide failures.

This comprehensive guide walks you through the process of implementing a complete backup and restore strategy for Longhorn volumes using AWS S3 as the backup target. You'll learn not only how to configure S3-based backups, but also understand the critical distinctions between high availability, snapshots, and backups—and why all three are essential for production workloads. By the end of this guide, you'll have a robust data protection strategy that safeguards against infrastructure failures, operational mistakes, and catastrophic disasters.

== Prerequisites

Before implementing S3-based backups for your Longhorn volumes, ensure you have the following components and access levels in place:

*   **Longhorn Installed**: A fully operational Kubernetes cluster with Longhorn storage system deployed and running. This guide targets Longhorn v1.11.0 or later, which includes enhanced backup features, improved S3 integration, and critical bug fixes related to snapshot consistency. Earlier versions may work but could lack important functionality or contain known issues. You can verify your Longhorn version by running `kubectl get deploy -n longhorn-system longhorn-manager -o jsonpath='{.spec.template.spec.containers[0].image}'`.

*   **AWS Account**: Active access to an AWS account with sufficient permissions to create and manage S3 buckets and IAM resources. You'll need either root account access or an IAM user with administrative permissions for S3 and IAM services. If working in an enterprise environment, coordinate with your cloud platform team to obtain the necessary access or have them create resources on your behalf.

*   **AWS CLI** (Optional but Recommended): The AWS Command Line Interface provides a convenient way to verify S3 bucket creation, test permissions, and troubleshoot connectivity issues. While all configuration can be done through the AWS Console, the CLI is invaluable for debugging. Install version 2.x for the latest features: `aws --version` should show version 2.0.0 or higher.

*   **kubectl**: The Kubernetes command-line tool must be installed and configured with administrative access to your target cluster. You'll need permissions to create secrets in the `longhorn-system` namespace and manage Longhorn custom resources. Verify your access with `kubectl auth can-i create secrets -n longhorn-system`—it should return "yes".

== Longhorn Snapshot and Backup

_"If Longhorn already keeps my volumes HA across 3 nodes / AZs, why do I need snapshots and backups at all?"_

**The key is this:**

*   **HA protects you from infrastructure failure.**
*   **Snapshots and backups protect you from everything else.**

They solve different classes of problems.

=== What Longhorn HA Actually Protects You From

Longhorn's High Availability ensures your data remains available despite **infrastructure-level failures**:

*   **Node failures**: If a Kubernetes node crashes, dies, or becomes unreachable.
*   **Disk failures**: When a physical disk fails on a node.
*   **Availability Zone outages**: If an entire AZ goes down (when replicas are spread across AZs).
*   **Pod evictions**: Normal cluster operations like node draining or autoscaling.

**How it works**: Longhorn creates multiple replicas (typically 3) of your volume data, each stored on a different node. If one replica becomes unavailable, the volume continues operating seamlessly using healthy replicas.

=== What HA Does Not Protect You From (Critical)

HA replication propagates **all changes** to all replicas—including destructive ones:

*   **Accidental deletion**: Delete a file or database record? **All replicas reflect the deletion immediately.**
*   **Data corruption**: Application bug corrupts data? **All replicas get the corrupted data.**
*   **Ransomware/malware**: Malicious encryption or data modification? **Replicated to all copies.**
*   **Bad migrations**: Database schema migration fails halfway? **HA can't roll it back.**
*   **Human error**: Wrong `kubectl delete`, misconfigured app, accidental overwrite? **Replicated everywhere.**
*   **Cluster-wide disasters**: Control plane failure, cluster misconfiguration, namespace deletion.

**Critical insight**: HA keeps your data available _as-is_. It doesn't protect you from _what_ the data becomes.

=== Where Snapshots Fit (Local, Fast, Operational)

Longhorn snapshots are **point-in-time copies stored locally within the cluster**.

**Use cases**:

*   **Pre-upgrade safety**: Take a snapshot before upgrading an application or database schema.
*   **Fast rollback**: Restore to a known-good state in seconds/minutes.
*   **Operational testing**: Clone volumes for testing without affecting production.
*   **Hourly/daily operational checkpoints**: Frequent snapshots with short retention.

**Characteristics**:

*   **Fast**: Snapshot creation and restoration happen in seconds.
*   **Local**: Stored within the Longhorn cluster (same infrastructure).
*   **Space-efficient**: Use delta/incremental storage.
*   **Limited lifespan**: Typically kept for hours to days.

**Limitation**: Snapshots are stored _in the same cluster_. If the cluster is lost or the Longhorn system itself is compromised, snapshots are lost too.

=== Where Backups Fit (Off-Cluster, Long-Term, DR)

Longhorn backups are **snapshots uploaded to external storage** (S3, NFS, etc.).

**Use cases**:

*   **Disaster recovery (DR)**: Recover from complete cluster failure.
*   **Cross-cluster migration**: Restore volumes in a different cluster.
*   **Compliance and retention**: Long-term data retention policies (weeks, months, years).
*   **Protection against cluster-level catastrophes**: Namespace deletion, control plane failure, complete infrastructure loss.

**Characteristics**:

*   **Off-cluster**: Stored outside the Kubernetes cluster (different infrastructure).
*   **Durable**: Survives cluster deletion, Longhorn uninstallation, or regional outages.
*   **Slower**: Restore operations may take minutes to hours depending on data size.
*   **Long retention**: Kept for weeks, months, or indefinitely.

**Critical**: Backups are your **last line of defense**. They're the only mechanism that survives a complete cluster failure.

=== How HA, Snapshots, and Backups Complement Each Other

Each layer addresses different failure domains:

[options="header"]
|===
|Mechanism |Protects Against |Recovery Time |Retention |Scope

|**HA Replicas**
|Node/disk/AZ failure
|Instant (automatic)
|Always active
|Infrastructure only

|**Snapshots**
|Accidental changes, bad deployments
|Seconds to minutes
|Hours to days
|In-cluster

|**Backups**
|Cluster failure, disasters, compliance
|Minutes to hours
|Weeks to years
|Off-cluster

|===

**Together**: HA keeps your service running, snapshots give you operational safety, and backups provide true disaster recovery.

=== Concrete Examples

==== Example 1: Airflow Logs

*   **HA**: Logs stay available if a node dies.
*   **Snapshot**: Rarely useful (logs are append-only).
*   **Backup**: Mostly unnecessary (logs are ephemeral and can be regenerated).

**Conclusion**: HA is sufficient; snapshots and backups are optional.

==== Example 2: Airflow Metadata Database

*   **HA**: Keeps the database running during node failures.
*   **Snapshot**: Taken before schema migrations or major configuration changes.
*   **Backup**: Recover from a failed migration, accidental data deletion, or cluster disaster.

**Conclusion**: All three mechanisms are needed. This is mission-critical stateful data.

==== Example 3: CI/CD Artifacts

*   **HA**: Ensures builds don't fail due to node loss.
*   **Snapshot**: Quick rollback after a bad pipeline deployment.
*   **Backup**: Optional, with short retention (artifacts can often be regenerated).

**Conclusion**: HA + snapshots for operational stability; backups optional depending on regeneration cost.

==== Example 4: Platform Configuration Data

_(Keycloak database, Argo CD state, internal tools)_

*   **HA**: Maintains uptime during infrastructure failures.
*   **Snapshot**: Enables safe upgrades and testing.
*   **Backup**: Required for disaster recovery and compliance (audit trails, user data).

**Conclusion**: All three are essential for production platforms.

=== Why "HA Only" Is Dangerous

Relying solely on HA replication exposes you to:

WARNING: **Unrecoverable data loss** from application bugs, human error, or malicious activity. Once data is corrupted or deleted, all replicas reflect that state immediately. Without snapshots or backups, there is no recovery path.

Real-world failure scenarios that HA cannot protect against:

*   A developer runs `kubectl delete pvc` on the wrong namespace.
*   A database migration script has a bug and corrupts critical tables.
*   Ransomware encrypts application data.
*   A misconfigured application truncates a production database table.

**All of these scenarios propagate instantly to all HA replicas.** Without snapshots or backups, your data is gone.

=== Practical Recommendations

A reasonable production approach:

[options="header"]
|===
|Data Type |HA |Snapshots |Backups

|Logs
|✅ Required
|❌ Not needed
|❌ Not needed

|Metadata Database
|✅ Required
|✅ Before migrations
|✅ Daily or more

|User/Application Data
|✅ Required
|⚠️ Recommended
|✅ Daily or more

|CI/CD Cache
|✅ Required
|❌ Not needed
|❌ Not needed

|Platform State
|✅ Required
|✅ Before upgrades
|✅ Daily or more

|===

NOTE: The exact snapshot and backup frequency depends on your RPO (Recovery Point Objective) and how much data loss is acceptable.

=== Summary

[quote]
____
Even with fully HA replicas across multiple availability zones, snapshots and backups remain essential. HA protects availability during infrastructure failures, snapshots enable safe operations and fast rollback, and backups provide true disaster recovery. These mechanisms solve different problems and must be used together for production environments.
____


== Step 1: Create an AWS S3 Bucket

The first step in establishing an off-cluster backup solution is creating a dedicated S3 bucket to serve as your backup target. This bucket will store all Longhorn volume backups, providing durable, geographically distributed storage that survives cluster failures, infrastructure outages, and regional disasters.

**Detailed Steps:**

1.  **Access the AWS S3 Console**:
    *  Log in to the **AWS Management Console** using your credentials.
    *  Navigate to the **S3** service (you can use the search bar at the top or find it under "Storage" in the services menu).

2.  **Initiate Bucket Creation**:
    *  Click the **Create bucket** button.

3.  **Configure Bucket Name**:
    *  Enter a globally unique **Bucket name**. S3 bucket names must be unique across all AWS accounts worldwide, so choose a descriptive name that includes your organization, environment, and purpose.
    *  Example: `acmecorp-prod-longhorn-backups` or `my-k8s-longhorn-backups-us-east-1`
    *  **Naming Best Practices**: Use lowercase letters, numbers, and hyphens only. Avoid using periods (which can cause SSL certificate issues) and keep names under 63 characters.

4.  **Select AWS Region**:
    *  Choose an **AWS Region** strategically. For optimal performance and reduced data transfer costs, select a region geographically close to your Kubernetes cluster.
    *  **Important**: For true disaster recovery, consider choosing a region *different* from where your cluster runs. This protects against regional outages.
    *  Example: If your cluster is in `us-east-1`, you might choose `us-east-1` for performance or `us-west-2` for geographic redundancy.

5.  **Configure Security Settings**:
    *  **Critical**: Leave **Block all public access** enabled (this should be the default). Your backup data should never be publicly accessible.
    *  S3 bucket-level encryption is optional but recommended. You can enable default encryption using AES-256 (SSE-S3) or AWS KMS for additional security.
    *  Versioning is optional for backup buckets—Longhorn manages its own versioning internally through backup metadata.

6.  **Review and Create**:
    *  Leave other advanced settings as default unless you have specific compliance or lifecycle management requirements.
    *  Click **Create bucket** to finalize.

TIP: **Document the following information** for later use: (1) **Bucket Name** (exact spelling), (2) **AWS Region** (e.g., `us-east-1`, not "US East"). You'll need these when configuring Longhorn's backup target.

== Step 2: Create an AWS IAM User and Policy

Longhorn needs permission to access the S3 bucket. It is best practice to create a dedicated IAM user with restricted permissions.

=== 2.1 Create an IAM Policy

An IAM policy defines the exact permissions that Longhorn will have when accessing your S3 bucket. Following the principle of least privilege, this policy grants only the minimum permissions necessary for backup and restore operations—nothing more.

**Detailed Steps:**

1.  **Access IAM Policies**:
    *  Navigate to **IAM** (Identity and Access Management) in the AWS Console.
    *  In the left sidebar, click **Policies**.
    *  Click the **Create policy** button.

2.  **Define Policy Using JSON**:
    *  Select the **JSON** tab (it provides more precision than the visual editor).
    *  Paste the following policy document, replacing `<your-bucket-name>` with your actual S3 bucket name.

.LonghornS3BackupPolicy.json
[source,json]
----
{
    "Version": "2012-10-17",  // IAM policy language version (always use this)
    "Statement": [
        {
            "Sid": "LonghornBackupAccess",  // Statement ID for documentation
            "Effect": "Allow",  // Grant permissions (as opposed to "Deny")
            "Action": [
                // Upload backup data to S3 (create/overwrite objects)
                "s3:PutObject",
                
                // Download backup data during restore operations
                "s3:GetObject",
                
                // List existing backups in the bucket (required for backup discovery)
                "s3:ListBucket",
                
                // Remove old backups based on retention policies
                "s3:DeleteObject"
            ],
            "Resource": [
                // Bucket-level operations (ListBucket requires this)
                "arn:aws:s3:::<your-bucket-name>",
                
                // Object-level operations (PutObject, GetObject, DeleteObject)
                // The /* wildcard allows access to all objects within the bucket
                "arn:aws:s3:::<your-bucket-name>/*"
            ]
        }
    ]
}
----

**Understanding the Permissions:**

* **`s3:PutObject`**: Allows Longhorn to upload snapshot data to S3. This is the core backup operation.
* **`s3:GetObject`**: Enables restoration by downloading backup data back to the cluster.
* **`s3:ListBucket`**: Required for Longhorn to enumerate existing backups and display them in the UI.
* **`s3:DeleteObject`**: Allows cleanup of old backups according to retention policies, preventing unlimited storage growth.

NOTE: This policy is intentionally restrictive. It grants no permissions to create buckets, modify bucket configurations, or access other buckets. If an attacker were to compromise these credentials, they could only affect this single bucket.

3.  **Name and Create the Policy**:
    *  Click **Next** (or **Next: Tags** if you want to add organizational tags).
    *  Enter a descriptive **Name**: `LonghornS3BackupPolicy`
    *  Optionally add a **Description**: "Grants Longhorn the minimum required permissions to backup and restore volumes to S3"
    *  Click **Create policy**.

=== 2.2 Create an IAM User

With the policy created, the next step is to create a dedicated IAM user that will use this policy. This user represents the "identity" that Longhorn will assume when interacting with S3.

WARNING: In production environments, consider using **IAM Roles for Service Accounts (IRSA)** instead of IAM users with static credentials. IRSA provides temporary, automatically rotated credentials and is more secure than long-lived access keys. However, for simplicity and compatibility with all Kubernetes distributions, this guide uses IAM users. See the IRSA implementation guide for production-grade authentication.

**Detailed Steps:**

1.  **Access IAM Users**:
    *  Navigate to **Users** in the IAM console.
    *  Click **Create user**.

2.  **Configure User Identity**:
    *  Enter a descriptive **User name**: `longhorn-backup-user`
    *  This name is purely for identification—it doesn't need to match anything in Kubernetes.
    *  Do **not** enable "Provide user access to the AWS Management Console"—this is a programmatic-only user with no login capabilities.

3.  **Click Next** to proceed to permissions.

4.  **Attach the Policy**:
    *  Select **Attach policies directly**.
    *  In the search box, type `LonghornS3BackupPolicy` (the policy you created earlier).
    *  Check the box next to your policy to select it.
    *  Verify that only this one policy is selected.

5.  **Review and Create**:
    *  Click **Next** to review.
    *  Verify the policy attachment is correct.
    *  Click **Create user** to finalize.

=== 2.3 Create Access Keys

Access keys consist of an Access Key ID and a Secret Access Key—think of them as a username and password for programmatic access to AWS. Longhorn will use these credentials to authenticate with S3.

**Security Considerations:**

* Access keys are long-lived credentials that don't expire automatically.
* The secret access key is shown only once during creation. If lost, you must delete and recreate the key.
* Keep these credentials secure—they grant full access to your backup bucket.
* For production environments, implement key rotation policies (e.g., rotate every 90 days).

**Detailed Steps:**

1.  **Access User Security Credentials**:
    *  Click on the newly created user name (`longhorn-backup-user`) to open the user details.
    *  Navigate to the **Security credentials** tab.

2.  **Initiate Key Creation**:
    *  Scroll down to the **Access keys** section.
    *  Click **Create access key**.

3.  **Select Use Case**:
    *  AWS will prompt you to select a use case. Choose **Application running outside AWS** or **Third-party service**.
    *  This selection helps AWS understand your intent but doesn't affect functionality.
    *  Check the acknowledgment box confirming you understand the security implications.
    *  Click **Next**.

4.  **Optional Description**:
    *  Add a description tag like "Longhorn S3 backup credentials for prod cluster" to help identify this key later.
    *  Click **Create access key**.

5.  **Retrieve and Secure Credentials**:
    *  **CRITICAL**: You are now viewing the credentials. This is your **only opportunity** to see the **Secret access key**.
    *  Copy both the **Access key** and **Secret access key** to a secure location (password manager, encrypted file, etc.).
    *  Optionally, click **Download .csv file** for a backup copy.
    *  **Never commit these credentials to Git** or store them in plain text in your infrastructure-as-code repositories.

TIP: Consider using a `.env` file (excluded from Git via `.gitignore`) or a secure secrets management system like AWS Secrets Manager, HashiCorp Vault, or Kubernetes external-secrets operator for credential storage in automated workflows.

== Step 3: Create a Kubernetes Secret

Now that you have AWS credentials, you need to make them available to Longhorn within your Kubernetes cluster. Kubernetes Secrets provide a secure way to store sensitive information like API keys, passwords, and credentials. Longhorn will read this secret to authenticate with S3 during backup and restore operations.

**Why use Secrets?**

* Separates sensitive data from application configuration
* Secrets are stored encrypted at rest in etcd (if cluster encryption is enabled)
* Secrets can be restricted using Kubernetes RBAC policies
* Secrets are never logged or exposed in pod specifications

=== Method 1: Create Secret Using kubectl (Recommended for Quick Setup)

This method creates the secret directly from the command line without creating intermediate files that might accidentally be committed to version control.

**Detailed Steps:**

1. **Set Environment Variables** (for convenience and to avoid exposing credentials in shell history):
+
[source,bash]
----
# Store your AWS credentials in environment variables
# These variables are temporary and only exist in your current shell session
export S3_BACKUP_AWS_ACCESS_KEY_ID=<YOUR_ACCESS_KEY_ID>
export S3_BACKUP_AWS_SECRET_ACCESS_KEY=<YOUR_SECRET_ACCESS_KEY>

# Verify they're set (this will display the values - be careful not to share your screen)
echo $S3_BACKUP_AWS_ACCESS_KEY_ID
----

2. **Create the Secret**:
+
[source,bash]
----
# Create a secret named 'aws-s3-backup-credentials' in the 'longhorn-system' namespace
# The secret contains two key-value pairs that Longhorn expects:
#   - AWS_ACCESS_KEY_ID: The access key for authentication
#   - AWS_SECRET_ACCESS_KEY: The secret key for authentication
kubectl create secret generic aws-s3-backup-credentials \
  --from-literal=AWS_ACCESS_KEY_ID=$S3_BACKUP_AWS_ACCESS_KEY_ID \
  --from-literal=AWS_SECRET_ACCESS_KEY=$S3_BACKUP_AWS_SECRET_ACCESS_KEY \
  -n longhorn-system
----
+
**Understanding the Command:**
+
* `kubectl create secret generic`: Creates a generic (opaque) secret
* `aws-s3-backup-credentials`: The name of the secret (you'll reference this in Longhorn configuration)
* `--from-literal=KEY=VALUE`: Creates secret data from literal values
* `-n longhorn-system`: Creates the secret in the Longhorn namespace where it's needed

3. **Verify Secret Creation**:
+
[source,bash]
----
# Check that the secret was created successfully
kubectl get secret aws-s3-backup-credentials -n longhorn-system

# View secret structure (values are base64-encoded, not plaintext)
kubectl describe secret aws-s3-backup-credentials -n longhorn-system
----

4. **Clear Environment Variables** (security hygiene):
+
[source,bash]
----
# Remove credentials from your shell environment
unset S3_BACKUP_AWS_ACCESS_KEY_ID
unset S3_BACKUP_AWS_SECRET_ACCESS_KEY
----

=== Method 2: Create Secret Using YAML (Recommended for GitOps)

For infrastructure-as-code workflows or when managing secrets declaratively, you can create a YAML manifest. **However, never commit credentials directly to Git.**

WARNING: The example below contains plaintext credentials using `stringData`. In production GitOps workflows, use sealed secrets, external-secrets operator, or a similar tool to encrypt credentials before committing to Git.

1. **Create a YAML file** named `aws-creds.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  # The secret name - must match what you configure in Longhorn backup target
  name: aws-s3-backup-credentials
  # Must be in the longhorn-system namespace
  namespace: longhorn-system
# Generic secrets use 'Opaque' type
type: Opaque
# Use 'stringData' for human-readable values (Kubernetes auto-converts to base64)
stringData:
  # AWS Access Key ID - identifies the IAM user
  AWS_ACCESS_KEY_ID: <YOUR_ACCESS_KEY_ID>
  # AWS Secret Access Key - authenticates the IAM user
  AWS_SECRET_ACCESS_KEY: <YOUR_SECRET_ACCESS_KEY>
----
+
NOTE: The keys `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` must be named exactly as shown. Longhorn expects these specific key names when reading S3 credentials.

2. **Apply the Secret to Your Cluster**:
+
[source,bash]
----
# Create the secret from the YAML file
kubectl apply -f aws-creds.yaml

# Verify creation
kubectl get secret aws-s3-backup-credentials -n longhorn-system
----

3. **Secure or Delete the YAML File**:
+
[source,bash]
----
# Option 1: Delete the file immediately after creation
rm aws-creds.yaml

# Option 2: If keeping for documentation, remove credential values
# and add to .gitignore to prevent accidental commits
echo "aws-creds.yaml" >> .gitignore
----

TIP: For production environments, consider using Kubernetes external-secrets operator, sealed-secrets, or AWS Secrets Manager integration to avoid storing plaintext credentials in YAML files.

== Step 4: Configure Backup Target in Longhorn

With the S3 bucket created and credentials stored in Kubernetes, the final configuration step is to tell Longhorn where to store backups and how to authenticate. This is done through the Longhorn UI by configuring the backup target.

**Understanding Backup Targets:**

A backup target is an external storage system (S3, NFS, etc.) where Longhorn uploads volume snapshots. Once configured, all backups from all volumes in the cluster can use this target. Longhorn supports multiple backup targets, but most deployments use a single "default" target.

**Detailed Configuration Steps:**

1.  **Access the Longhorn UI**:
    *  If you haven't already, forward the Longhorn frontend service to your local machine:
+
[source,bash]
----
# Forward Longhorn UI to http://localhost:8080
kubectl port-forward -n longhorn-system svc/longhorn-frontend 8080:80
----
+
    *  Open your browser and navigate to `http://localhost:8080`
    *  Alternatively, if Longhorn is exposed via Ingress, use the configured domain name.

2.  **Navigate to Backup Target Configuration**:
    *  In the Longhorn UI, click **Backup and Restore** in the top navigation menu.
    *  Select the **Backup Targets** tab.
    *  You should see a backup target named **default**.

3.  **Edit the Default Backup Target**:
    *  Click on the **default** backup target row.
    *  Click the **Edit** button (usually a pencil icon or "Edit" text).

4.  **Configure S3 Connection Details**:
    *  In the **Edit Backup Target default** dialog, fill in the following fields:
+
[options="header"]
|===
|Field |Value |Description

|**Backup Target**
|`s3://<bucket-name>@<region>/`
|S3 URL format. Replace `<bucket-name>` with your bucket name (e.g., `my-longhorn-backups`) and `<region>` with the AWS region (e.g., `us-east-1`). The trailing `/` is important.

|**Credential Secret**
|`aws-s3-backup-credentials`
|Select the Kubernetes secret name you created in Step 3. This dropdown should auto-populate with secrets in the `longhorn-system` namespace.

|**Poll Interval**
|`300` (default)
|How often (in seconds) Longhorn checks for new or updated backups in S3. 300 seconds (5 minutes) is typically sufficient.

|===
+
**Example S3 Backup Target URL:**
+
----
s3://acmecorp-prod-longhorn-backups@us-east-1/
s3://my-k8s-backups@eu-west-1/
s3://longhorn-backup-bucket@ap-southeast-1/
----
+
IMPORTANT: The URL format is strict. Common mistakes include: forgetting the `@` symbol, omitting the region, forgetting the trailing `/`, or using the bucket ARN instead of the bucket name.

5.  **Save and Test the Configuration**:
    *  Click **OK** to save the changes.
    *  Longhorn will immediately attempt to connect to S3 using the provided credentials.
    *  **Success indicators**:
      **  A green checkmark appears next to the backup target
      **  No error messages are displayed
      **  The "State" shows as "Ready" or "Available"
    *  **Failure indicators**:
      **  Red error icon or error message
      **  The "State" shows as "Error" or "Unavailable"
      **  Check the troubleshooting section below for common issues.

6.  **Verify Connectivity** (Optional but Recommended):
+
[source,bash]
----
# Check Longhorn manager logs for connection details
kubectl logs -n longhorn-system -l app=longhorn-manager --tail=50 | grep -i backup

# Look for successful S3 connection messages or error details
----



== Step 5: Verify Configuration

Before relying on backups for production data, it's critical to verify that the entire backup pipeline works correctly. This verification process ensures that snapshots can be created, uploaded to S3, and retrieved successfully.

**Comprehensive Verification Steps:**

1.  **Navigate to the Volume Page**:
    *  In the Longhorn UI, click **Volume** in the top navigation menu.
    *  You should see a list of all persistent volumes managed by Longhorn.

2.  **Select a Test Volume**:
    *  Choose a volume for testing. For initial verification, use a non-critical volume if possible.
    *  Click on the volume name to open its detail page.

3.  **Access Snapshot Management**:
    *  In the volume detail view, click on the **Snapshots and Backups** tab.
    *  This tab shows all local snapshots and allows you to create new ones.

4.  **Create a Test Snapshot**:
    *  If no snapshots exist, click **Take Snapshot**.
    *  Optionally provide a name or label (e.g., `test-backup-verification`).
    *  The snapshot should appear in the list within a few seconds.
    *  **What's happening**: Longhorn is creating a point-in-time copy of the volume data and storing it locally within the cluster.

5.  **Create a Backup from the Snapshot**:
    *  Locate the snapshot you just created in the snapshot list.
    *  Click the **Create Backup** button for that snapshot (may be labeled "Backup" or have a cloud/upload icon).
    *  Optionally add labels to the backup for easier identification (e.g., `environment=test`, `purpose=verification`).
    *  Click **OK** to initiate the backup.
    *  **What's happening**: Longhorn is uploading the snapshot data to your S3 bucket. Depending on volume size and network speed, this may take anywhere from a few seconds to several minutes.

6.  **Monitor Backup Progress**:
    *  The backup will show a progress indicator while uploading.
    *  Once complete, the status will change to "Completed" or show a checkmark.

7.  **Verify Backup in Longhorn UI**:
    *  Click **Backup and Restore** in the top menu.
    *  Select the **Backups** tab.
    *  You should see your volume listed under the **Volume** tab.
    *  Click on the volume name to see all backups for that volume.
    *  Verify that your test backup appears with the correct timestamp and labels.

8.  **Verify Backup in S3 Console** (Optional but Recommended):
    *  Log into the AWS S3 Console.
    *  Navigate to your backup bucket (e.g., `my-longhorn-backups`).
    *  You should see a directory structure created by Longhorn:
+
----
my-longhorn-backups/
├── backups/
│   └── <volume-name>/
│       ├── backup-<timestamp>.cfg  # Backup metadata
│       └── blocks/                 # Actual data blocks
----
+
    *  The presence of these files confirms that Longhorn successfully uploaded data to S3.

TIP: If the backup completes successfully in the Longhorn UI and you can see files in S3, your backup configuration is working correctly. This verification should be performed after initial setup and periodically thereafter.

== Troubleshooting

Backup configuration failures usually fall into one of three categories: permissions issues, network connectivity problems, or configuration errors. Here's how to diagnose and fix the most common issues:

=== Error: "Access Denied" or "AWS Error: Access Denied"

**Symptom**: Longhorn cannot connect to S3, or backup creation fails with an access denied error.

**Root Causes:**

1. **Incorrect IAM Policy**: The policy doesn't grant all required permissions.
2. **Wrong Bucket ARN**: The policy references a different bucket than configured.
3. **Invalid Credentials**: The access key or secret key is incorrect or has been rotated.

**Resolution Steps:**

1.  **Verify IAM Policy Permissions**:
+
[source,bash]
----
# Use AWS CLI to check the user's attached policies
aws iam list-attached-user-policies --user-name longhorn-backup-user

# Verify the policy has all four required permissions:
# s3:PutObject, s3:GetObject, s3:ListBucket, s3:DeleteObject
aws iam get-policy-version \
  --policy-arn <policy-arn-from-previous-command> \
  --version-id v1
----

2.  **Verify Bucket Name in Policy**:
    *  Ensure the bucket name in the IAM policy **exactly matches** the actual S3 bucket name.
    *  Check for typos, extra spaces, or case sensitivity issues (bucket names are case-sensitive).

3.  **Test Credentials Manually**:
+
[source,bash]
----
# Extract credentials from the Kubernetes secret
AWS_ACCESS_KEY_ID=$(kubectl get secret aws-s3-backup-credentials -n longhorn-system -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d)
AWS_SECRET_ACCESS_KEY=$(kubectl get secret aws-s3-backup-credentials -n longhorn-system -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 -d)

# Test listing the bucket using the credentials
AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
aws s3 ls s3://<your-bucket-name>/

# If this fails, the credentials or permissions are incorrect
----

4.  **Recreate Secret with Correct Credentials**:
    *  If credentials were rotated or incorrect, delete and recreate the secret:
+
[source,bash]
----
kubectl delete secret aws-s3-backup-credentials -n longhorn-system
# Then recreate following Step 3 instructions
----

=== Error: "Unable to List S3 Bucket" or "Invalid Endpoint"

**Symptom**: Longhorn reports that it cannot list or access the S3 bucket.

**Root Causes:**

1. **Region Mismatch**: The region in the backup target URL doesn't match the bucket's actual region.
2. **Incorrect URL Format**: The S3 URL is malformed.
3. **Bucket Doesn't Exist**: The bucket name is incorrect or the bucket was deleted.

**Resolution Steps:**

1.  **Verify Bucket Region**:
+
[source,bash]
----
# Check the bucket's actual region
aws s3api get-bucket-location --bucket <your-bucket-name>

# Output will be: {"LocationConstraint": "us-east-1"}
# (Note: us-east-1 buckets may return null, which is correct)
----

2.  **Verify Backup Target URL Format**:
    *  Correct format: `s3://<bucket-name>@<region>/`
    *  Common mistakes:
      **  Missing `@` symbol: `s3://<bucket-name>/<region>/` ❌
      **  Missing trailing `/`: `s3://<bucket-name>@<region>` ❌
      **  Using ARN: `s3://arn:aws:s3:::<bucket-name>@<region>/` ❌
      **  Wrong region: `s3://my-bucket@us-west-2/` when bucket is in `us-east-1` ❌

3.  **Update Backup Target with Correct Region**:
    *  Edit the backup target in Longhorn UI with the correct region from step 1.

=== Error: "Backup Target Not Ready" or "Secret Not Found"

**Symptom**: The backup target shows as unavailable or Longhorn can't find the credential secret.

**Root Causes:**

1. **Secret Doesn't Exist**: The secret wasn't created or was created in the wrong namespace.
2. **Secret Name Mismatch**: The secret name in Longhorn config doesn't match the actual secret name.
3. **Wrong Namespace**: The secret exists but not in `longhorn-system` namespace.

**Resolution Steps:**

1.  **Verify Secret Exists**:
+
[source,bash]
----
# Check if secret exists in longhorn-system namespace
kubectl get secret aws-s3-backup-credentials -n longhorn-system

# If not found, verify it wasn't created in a different namespace
kubectl get secrets --all-namespaces | grep aws-s3-backup-credentials
----

2.  **Verify Secret Has Correct Keys**:
+
[source,bash]
----
# The secret must contain exactly these two keys:
# - AWS_ACCESS_KEY_ID
# - AWS_SECRET_ACCESS_KEY
kubectl describe secret aws-s3-backup-credentials -n longhorn-system
----

3.  **Recreate Secret in Correct Namespace**:
    *  If the secret is in the wrong namespace or missing keys, delete and recreate it following Step 3.

=== Additional Troubleshooting Commands

[source,bash]
----
# View Longhorn manager logs for detailed error messages
kubectl logs -n longhorn-system -l app=longhorn-manager --tail=100

# Check backup controller logs specifically
kubectl logs -n longhorn-system deploy/longhorn-manager -c longhorn-manager | grep -i backup

# Verify network connectivity from a pod to S3
kubectl run -it --rm debug --image=amazon/aws-cli --restart=Never -- \
  s3 ls s3://<your-bucket-name>/ --region <your-region>
----

NOTE: Most backup configuration issues can be resolved by carefully verifying: (1) IAM permissions, (2) bucket region, (3) secret existence and correctness, and (4) backup target URL format.


== How to Create a Backup

Understanding Longhorn's backup architecture is key to effective data protection. Longhorn follows a snapshot-first approach: **backups are always created from snapshots, not directly from live volumes**. This design ensures data consistency by capturing a stable, point-in-time state before uploading to external storage.

**Why Snapshot-Based Backups?**

* **Consistency**: Snapshots freeze the volume state at a specific moment, preventing data corruption from ongoing writes.
* **Efficiency**: Longhorn uses incremental snapshots, so only changed data blocks are stored.
* **Flexibility**: You can create multiple snapshots locally (fast) and selectively choose which ones to backup externally (slower but durable).

**Backup Creation Workflow:**

1. **Snapshot Creation**: Capture the current volume state as a local snapshot.
2. **Backup Upload**: Upload the snapshot data to the configured backup target (S3, NFS, etc.).
3. **Metadata Storage**: Store backup metadata in S3 for discovery and restoration.

There are two approaches to creating backups: manual (on-demand) and automated (scheduled).

=== Manual Backup Creation

Manual backups are ideal for one-time backup needs, such as before major application upgrades, database migrations, or when testing new configurations. This gives you complete control over when backups are created.

**Detailed Workflow:**

1.  **Navigate to Volume Management**:
    *  Open the Longhorn UI.
    *  Click **Volume** in the top navigation menu.
    *  You'll see a list of all persistent volumes in your cluster.

2.  **Select Target Volume**:
    *  Click on the name of the volume you want to back up (e.g., `pvc-abc123` or `postgres-data`).
    *  This opens the volume detail page showing metadata, health status, and replicas.

3.  **Access Snapshot Management**:
    *  Click on the **Snapshots and Backups** tab.
    *  This shows all existing snapshots for this volume.

4.  **Create a Snapshot** (if you need a new one):
    *  Click the **Take Snapshot** button.
    *  **Optional**: Provide labels for easier identification:
      **  `name`: Descriptive name like `pre-migration-v2.0`
      **  `purpose`: Context like `before-schema-change`
      **  `created-by`: Your name or automation source
    *  Click **OK**.
    *  **What happens**: Longhorn creates a read-only, space-efficient copy of the volume's current state. This happens in seconds and doesn't interrupt the running application.
    *  The snapshot appears in the snapshot list immediately.

5.  **Create a Backup from the Snapshot**:
    *  In the snapshot list, locate the snapshot you want to back up.
    *  Click the **Create Backup** button for that snapshot (may have a cloud/upload icon or appear in a dropdown menu).
    *  **Optional**: Add backup-specific labels for retention policies or compliance tracking:
      **  `retention`: `7-days`, `30-days`, `permanent`
      **  `environment`: `production`, `staging`
      **  `compliance`: `gdpr`, `hipaa`
    *  Click **OK** to start the backup.
    *  **What happens**: Longhorn uploads the snapshot data to S3. Progress is shown in the UI. Depending on the volume size and network bandwidth, this can take from seconds to hours.

6.  **Monitor Backup Progress**:
    *  The backup will show a progress indicator (percentage or status message).
    *  You can navigate away—the backup continues in the background.
    *  Once complete, the backup status changes to "Completed" with a timestamp.

7.  **Verify Backup Success**:
    *  Click **Backup and Restore** in the top menu.
    *  Select the **Backups** tab.
    *  Under the **Volume** tab, find your volume.
    *  Your backup should be listed with the correct timestamp, size, and labels.

TIP: Manual backups are perfect for critical moments like before database schema changes, application upgrades, or configuration modifications. Always verify the backup completed successfully before proceeding with risky operations.

=== Automated Backup (Recurring Job)

For production environments, manual backups are insufficient—you need automated, scheduled backups that run without human intervention. Longhorn's recurring job feature provides cron-based scheduling for automatic backup creation.

**Key Advantage**: Recurring backup jobs automatically create a snapshot before uploading to S3, so you don't need separate snapshot and backup jobs.

**Detailed Configuration Steps:**

1.  **Navigate to Volume Management**:
    *  Open the Longhorn UI.
    *  Click **Volume** in the top navigation menu.

2.  **Select Volumes for Automation**:
    *  **Option A**: Select a single volume by clicking on its name.
    *  **Option B**: Select multiple volumes using checkboxes to apply the same backup schedule to all.

3.  **Create Recurring Job**:
    *  Click **Create Recurring Job** (or **Operations** → **Create Recurring Job**).

4.  **Configure Job Settings**:
+
[options="header"]
|===
|Field |Description |Example

|**Task**
|Select **Backup** (not "Snapshot"). This automatically creates a snapshot and uploads it to S3.
|`Backup`

|**Schedule (Cron Expression)**
|Cron format: `minute hour day month weekday`. Use cron generator tools if needed.
|`0 2 * * *` (daily at 2:00 AM UTC) +
`0 */6 * * *` (every 6 hours) +
`0 0 * * 0` (weekly on Sunday at midnight)

|**Retain**
|Number of backups to keep. Older backups are automatically deleted when this limit is exceeded.
|`7` (keep last 7 backups) +
`30` (keep 30 backups for ~monthly retention if running daily)

|**Concurrency**
|How many backups can run simultaneously. Leave as 1 for most cases to avoid resource contention.
|`1`

|**Labels** (Optional)
|Key-value pairs for organizing backups (e.g., filtering, retention policies).
|`environment=production` +
`scheduled=true`

|===

5.  **Save the Recurring Job**:
    *  Click **Save** or **OK**.
    *  The job is now active and will run according to the cron schedule.

IMPORTANT: When the `Backup` task runs, Longhorn **automatically creates a new snapshot first**, then uploads it to S3. You do not need to create separate recurring jobs for snapshots—the backup job handles everything.

6.  **Verify Recurring Job**:
    *  In the volume detail page, scroll to the **Recurring Jobs** section.
    *  Your job should be listed with status "Enabled" or "Active".
    *  The "Next Run" timestamp shows when the next backup will occur.

7.  **Monitor Automated Backups**:
    *  As scheduled backups complete, they appear in **Backup and Restore** → **Backups**.
    *  Check periodically to ensure backups are completing successfully.
    *  Set up alerting (e.g., Prometheus + Alertmanager) to notify you if backups fail.

**Cron Schedule Examples:**

[source,text]
----
# Every day at 2:00 AM UTC
0 2 * * *

# Every 6 hours
0 */6 * * *

# Every Sunday at midnight (weekly backups)
0 0 * * 0

# Every day at 3:00 AM on weekdays only (Mon-Fri)
0 3 * * 1-5

# First day of every month at 1:00 AM
0 1 1 * *
----

TIP: Choose retention counts based on your Recovery Point Objective (RPO). For example, if running daily backups with retention of 30, you can restore to any point in the last 30 days. Balance retention needs against storage costs.

== Making Changes After Backup (Example: PostgreSQL Table Creation)

After creating a backup or snapshot, you may want to make changes to your application data to simulate real-world scenarios or test restore procedures. This example demonstrates creating a test table in PostgreSQL that you can use to verify restore operations later.

**Why This is Useful:**

* **Restore Verification**: After restoring from a backup, you can check whether this table exists or not to confirm you're using the correct restore point.
* **Point-in-Time Testing**: By creating distinct data before and after snapshots/backups, you can validate that restores return to the expected state.
* **Compliance Testing**: Demonstrate that backup and restore procedures work correctly for audit purposes.

**Connecting to PostgreSQL:**

First, connect to your PostgreSQL pod:

[source,bash]
----
# Connect to the PostgreSQL pod (adjust pod name and namespace as needed)
kubectl exec -it postgresql-0 -n service-foundry -- psql -U postgres

# Or specify a specific database
kubectl exec -it postgresql-0 -n service-foundry -- psql -U postgres -d mydatabase
----

**SQL Commands to Create Test Data:**

Run the following SQL commands within the PostgreSQL session to create a table and insert test data:

[source,sql]
----
-- Create a test table for backup/restore verification
-- IF NOT EXISTS prevents errors if the table already exists from a previous test
CREATE TABLE IF NOT EXISTS public.users_for_backup_testing
(
    -- User's full name (variable-length string up to 120 characters)
    name character varying(120) COLLATE pg_catalog."default",
    
    -- User's email address (variable-length string up to 120 characters)
    email character varying(120) COLLATE pg_catalog."default"
);

-- Insert sample data for verification
-- After restore, you can query this table to see if you're at the correct point in time
INSERT INTO users_for_backup_testing 
VALUES 
    -- Sample user 1
    ('kim', 'kim@company.com'), 
    
    -- Sample user 2
    ('lee', 'lee@company.com');

-- Verify the data was inserted correctly
SELECT * FROM users_for_backup_testing;
----

**Expected Output:**

[source,text]
----
 name |       email        
------+--------------------
 kim  | kim@company.com
 lee  | lee@company.com
(2 rows)
----

**Using This for Restore Testing:**

1. **Before backup**: Run these SQL commands to create the table with 2 rows.
2. **Create backup**: Create a snapshot and backup using Longhorn.
3. **After backup**: Add more data or modify the table:
+
[source,sql]
----
-- Add data AFTER the backup
INSERT INTO users_for_backup_testing VALUES ('park', 'park@company.com');

-- Now the table has 3 rows
SELECT count(*) FROM users_for_backup_testing;  -- Should return 3
---- 
+
4. **Restore from backup**: Follow the restore procedures in the next section.
5. **Verify restore**: After restoration, query the table:
+
[source,sql]
----
-- If restore was successful, you should see only the original 2 rows
SELECT count(*) FROM users_for_backup_testing;  -- Should return 2
SELECT * FROM users_for_backup_testing;  -- Should NOT include 'park'
----

TIP: Use different table names or column values for different test scenarios. For example, add a `created_at` timestamp column to precisely identify when data was inserted relative to snapshots and backups.

== Restore from Snapshot

Longhorn snapshots provide the fastest path to recovery from operational mistakes, bad deployments, or data corruption. Because snapshots are stored **locally within the cluster**, restoration happens in seconds to minutes rather than hours. This makes snapshots ideal for quick rollback scenarios.

**When to Use Snapshot Restore:**

* Immediately after a bad application deployment
* Following accidental data deletion or corruption
* After a failed database migration
* When testing changes before committing to production

**Important Limitations:**

* Snapshots only exist within your cluster—if the cluster is lost, snapshots are lost too
* Snapshots cannot be used for cross-cluster migrations
* They don't protect against cluster-wide failures

There are two methods for restoring from snapshots, each suited to different scenarios.

=== Method 1: Revert to Snapshot (In-Place Restore)

This method **overwrites** the existing volume with data from a previous snapshot. It's the fastest restoration method but is destructive—any changes made after the snapshot was taken are permanently lost.

WARNING: This operation is **irreversible**. All data written to the volume after the snapshot was taken will be **permanently deleted**. Always verify you're reverting to the correct snapshot before proceeding.

**When to Use In-Place Revert:**

* You're certain the current data is corrupted or unwanted
* Fast recovery is critical (seconds vs. minutes)
* You don't need to inspect the restored data before committing
* The application can tolerate brief downtime

**Detailed Workflow:**

**Step 1: Prepare Environment Variables**

Set up variables for easier command execution and to avoid typos:

[source,bash]
----
# Application configuration
APP_NAME=postgresql-app           # ArgoCD application name
NAMESPACE=service-foundry         # Kubernetes namespace where app is deployed

# Workload details
STATEFULSET_NAME=postgresql       # StatefulSet managing the pod
POD_NAME=postgresql-0             # Pod name (StatefulSet pods are predictable)

# Storage details
PVC_NAME=data-postgresql-0        # PVC name (matches StatefulSet volume claim template)
----

**Step 2: Disable ArgoCD Auto-Sync (If Using ArgoCD)**

If your application is managed by ArgoCD, you must disable auto-sync to prevent ArgoCD from recreating resources you're intentionally scaling down:

[source,bash]
----
# Remove sync policy (disables automated sync and self-healing)
# This prevents ArgoCD from interfering with manual operations
kubectl patch application ${APP_NAME} -n argocd \
  --type merge \
  -p '{"spec":{"syncPolicy":null}}'

# Verify sync policy was removed
kubectl get application ${APP_NAME} -n argocd -o jsonpath='{.spec.syncPolicy}'
# Should output empty or null
----

**Step 3: Stop the Application**

The volume must be detached before reverting. Scale down the application to zero replicas:

[source,bash]
----
# Scale down StatefulSet to 0 replicas
# This gracefully shuts down pods and detaches the PVC
kubectl scale statefulset ${STATEFULSET_NAME} --replicas=0 -n ${NAMESPACE}

# Wait for pod termination (timeout after 60 seconds)
kubectl wait --for=delete pod/${POD_NAME} -n ${NAMESPACE} --timeout=60s

# Verify no pods are running
kubectl get pods -n ${NAMESPACE} -l app=${STATEFULSET_NAME}
# Should show no pods or "No resources found"
----

**Step 4: Revert to Snapshot in Longhorn UI**

1.  **Navigate to Volume Management**:
    *  Open the Longhorn UI.
    *  Click **Volume** in the top navigation.

2.  **Select the Target Volume**:
    *  Find the volume corresponding to your PVC (usually named `pvc-<uid>`).
    *  If unsure which volume corresponds to which PVC, run:
+
[source,bash]
----
# Find the Longhorn volume name for your PVC
kubectl get pvc ${PVC_NAME} -n ${NAMESPACE} -o jsonpath='{.spec.volumeName}'
----

3.  **Verify Volume State**:
    *  Click on the volume name to open details.
    *  Check that the **State** is **Detached**.
    *  If still "Attached", wait a few more seconds or verify pods are fully terminated.

4.  **Enable Maintenance Mode**:
    *  Click the **Attach** button (or **Attach Volume** in the dropdown).
    *  **Enable "Maintenance Mode"** checkbox.
    *  Maintenance mode attaches the volume in read-write mode without mounting it to a node, allowing safe revert operations.
    *  Click **OK** to attach in maintenance mode.

5.  **Access Snapshot Management**:
    *  Click the **Snapshots and Backups** tab.
    *  You'll see a list of all snapshots for this volume.

6.  **Select and Revert Snapshot**:
    *  Locate the snapshot you want to revert to (check timestamp and labels carefully).
    *  Click the **Revert** button or icon for that snapshot.
    *  **Read the confirmation dialog carefully**—this operation cannot be undone.
    *  Confirm the revert operation.
    *  **What happens**: Longhorn overwrites the current volume data with the snapshot data. This typically completes in seconds to minutes depending on volume size.

7.  **Detach the Volume**:
    *  Once revert completes, the volume is still attached in maintenance mode.
    *  Click **Detach** to release the volume.
    *  Wait for the state to change back to **Detached**.

**Step 5: Restart the Application**

With the volume now containing the restored snapshot data, restart the application:

[source,bash]
----
# Scale StatefulSet back to 1 replica
# The pod will mount the PVC, which now contains the reverted data
kubectl scale statefulset ${STATEFULSET_NAME} --replicas=1 -n ${NAMESPACE}

# Wait for pod to be ready (timeout after 300 seconds)
kubectl wait --for=condition=ready pod/${POD_NAME} -n ${NAMESPACE} --timeout=300s

# Verify pod is running
kubectl get pod ${POD_NAME} -n ${NAMESPACE}
----

**Step 6: Re-enable ArgoCD Auto-Sync (If Applicable)**

Restore ArgoCD automated sync and self-healing:

[source,bash]
----
# Re-enable automated sync with prune and self-heal
kubectl patch application ${APP_NAME} -n argocd \
  --type merge \
  -p '{"spec":{"syncPolicy":{"automated":{"prune":true,"selfHeal":true}}}}'

# Verify sync policy was restored
kubectl get application ${APP_NAME} -n argocd -o jsonpath='{.spec.syncPolicy}'
----

**Step 7: Verify Data Restoration**

Confirm the data matches the expected snapshot state:

[source,bash]
----
# For PostgreSQL example:
kubectl exec -it ${POD_NAME} -n ${NAMESPACE} -- psql -U postgres -c "
  SELECT count(*) FROM users_for_backup_testing;
"
# Should show 2 if you're testing with the earlier example data
----

**Use Cases:**

*   **Quick rollback** after a bad deployment or configuration change
*   **Undo accidental data changes** (e.g., wrong DELETE query executed)
*   **Restore to pre-upgrade state** after a failed application or database upgrade

=== Method 2: Create Volume from Snapshot (Clone and Test)

This method creates a **new, independent volume** from a snapshot while leaving the original volume completely untouched. This non-destructive approach allows you to inspect and validate the restored data before committing to use it in production.

**When to Use Volume Cloning:**

* You want to test the restored data before replacing the production volume
* You need to keep the current data as a safety net
* You're creating a development/testing environment from production data
* You want to compare current data with a previous state

**Detailed Workflow:**

**Step 1: Clone Volume from Snapshot**

1.  **Navigate to Volume Management**:
    *  Open the Longhorn UI.
    *  Click **Volume** in the top navigation.

2.  **Select Source Volume**:
    *  Find and click the volume containing the snapshot you want to clone from.

3.  **Access Snapshot List**:
    *  Click the **Snapshots and Backups** tab.
    *  Review the list of available snapshots.

4.  **Initiate Clone Operation**:
    *  Click on the snapshot you want to clone to open the action menu.
    *  Select **Clone Volume** from the dropdown.

5.  **Configure New Volume**:
    *  Enter a descriptive name for the new volume (e.g., `postgres-data-restored-2024-02-02`).
    *  **Naming tip**: Include date or purpose in the name for easier identification.
    *  Click **OK** to create the clone.
    *  **What happens**: Longhorn creates a new, independent volume initialized with the snapshot data. This typically completes in minutes.

**Step 2: Create PVC for the Cloned Volume**

The cloned volume exists in Longhorn but isn't yet accessible to Kubernetes. You must create a PersistentVolume (PV) and PersistentVolumeClaim (PVC) to expose it:

1.  **Return to Volume List**:
    *  Click **Volume** in the top navigation.
    *  Find the newly created volume (it will have the name you specified).

2.  **Create PV/PVC**:
    *  Click on the new volume to open its details.
    *  Click **Create PV/PVC** (usually in the top action bar).

3.  **Configure PVC**:
    *  **PVC Name**: Enter a name (e.g., `data-postgresql-restored`).
    *  **Namespace**: Select the namespace where your application runs (e.g., `service-foundry`).
    *  **Storage Class**: Should pre-populate with `longhorn` (usually correct).
    *  Click **OK** to create the PV and PVC.

4.  **Verify PVC Creation**:
+
[source,bash]
----
# Verify the new PVC was created and is bound
kubectl get pvc data-postgresql-restored -n service-foundry

# Output should show STATUS: Bound
# NAME                        STATUS   VOLUME           CAPACITY   ACCESS MODES   STORAGECLASS
# data-postgresql-restored    Bound    postgres-data... 10Gi       RWO            longhorn
----

**Step 3: Test with the Cloned PVC (Optional)**

Before replacing your production PVC, test the cloned data:

[source,bash]
----
# Create a temporary pod to inspect the cloned data
kubectl run test-restore-pod \
  --image=postgres:14 \
  --restart=Never \
  -n service-foundry \
  --overrides='
{
  "spec": {
    "containers": [{
      "name": "postgres",
      "image": "postgres:14",
      "command": ["sleep", "3600"],
      "volumeMounts": [{
        "name": "data",
        "mountPath": "/var/lib/postgresql/data"
      }]
    }],
    "volumes": [{
      "name": "data",
      "persistentVolumeClaim": {
        "claimName": "data-postgresql-restored"
      }
    }]
  }
}'

# Wait for pod to be ready
kubectl wait --for=condition=ready pod/test-restore-pod -n service-foundry --timeout=60s

# Inspect the data
kubectl exec -it test-restore-pod -n service-foundry -- \
  psql -U postgres -c "SELECT count(*) FROM users_for_backup_testing;"

# Clean up test pod when done
kubectl delete pod test-restore-pod -n service-foundry
----

**Step 4: Update Application to Use Cloned PVC (If Satisfied)**

Once you've verified the cloned data is correct, update your application to use it:

**Option A: Edit StatefulSet (Requires Recreation)**

[source,bash]
----
# StatefulSets have immutable volume claim templates, so you must:
# 1. Scale down to 0
# 2. Delete the StatefulSet (not cascade delete - keeps pods)
# 3. Update the volumeClaimTemplate
# 4. Recreate the StatefulSet

# This is complex - see Option B for a simpler approach
----

**Option B: Update PVC Manually (Safer)**

1. Stop the application
2. Delete the old PVC (or rename it for safety)
3. Rename the cloned PVC to match the original name
4. Restart the application

[source,bash]
----
# Scale down application
kubectl scale statefulset postgresql --replicas=0 -n service-foundry
kubectl wait --for=delete pod/postgresql-0 -n service-foundry --timeout=60s

# Rename old PVC for safety (optional)
kubectl patch pvc data-postgresql-0 -n service-foundry \
  -p '{"metadata":{"name":"data-postgresql-0-backup"}}'

# Rename cloned PVC to match StatefulSet expectation
# Note: PVCs can't be renamed directly - you'd need to recreate
# See Longhorn documentation for PVC replacement procedures
----

NOTE: Replacing PVCs in StatefulSets is complex. For production use, consider creating a parallel StatefulSet with the new PVC, testing it, then switching traffic, or consult Longhorn's volume migration documentation.

**Use Cases:**

*   **Testing restored data** before committing to production use
*   **Creating development/staging environments** from production snapshots
*   **Keeping the original data** as a safety net during risky operations
*   **Comparing current vs. previous data** for debugging or auditing

=== Snapshot vs Backup: When to Use Each

Understanding when to use snapshots versus backups is crucial:

[options="header"]
|===
|Aspect |Snapshot Restore |Backup Restore

|**Speed**
|Very fast (seconds)
|Slower (minutes to hours)

|**Location**
|In-cluster only
|External storage (S3, NFS)

|**Storage**
|Local Longhorn storage
|Off-cluster backup target

|**Survives cluster deletion**
|❌ No
|✅ Yes

|**Cross-cluster restore**
|❌ No
|✅ Yes

|**Use Case**
|Operational rollbacks, testing
|Disaster recovery, long-term retention

|**When to Use**
|Before upgrades, quick undo
|DR, compliance, cluster migration

|===

=== Best Practices

1.  **Use snapshots** for:
    *   Pre-upgrade safety nets
    *   Quick operational rollbacks
    *   Testing changes before committing
    *   Frequent checkpoints (hourly/daily)

2.  **Use backups** for:
    *   Disaster recovery
    *   Long-term retention (weeks/months)
    *   Cross-cluster migrations
    *   Compliance requirements

3.  **Combined approach**:
    *   Take a snapshot before risky operations
    *   Schedule regular backups to external storage
    *   Test restore procedures regularly

TIP: Snapshots are your first line of defense for operational mistakes. Backups are your last line of defense for catastrophic failures.

== Restoring from Backup

Backup restoration is fundamentally different from snapshot restoration. While snapshots provide quick, in-cluster recovery, backups enable **cross-cluster restoration** and **disaster recovery** from complete cluster failures. When you restore a backup, Longhorn downloads data from your S3 bucket (or other backup target) and creates a new volume in your cluster.

**When to Use Backup Restore:**

* **Disaster recovery**: Your cluster has been completely lost or corrupted
* **Cross-cluster migration**: Moving data to a different Kubernetes cluster
* **Long-term recovery**: Restoring data from weeks or months ago (beyond snapshot retention)
* **Cluster rebuild**: Recreating volumes after infrastructure replacement
* **Compliance**: Recovering data for audit or regulatory requirements

**Key Differences from Snapshot Restore:**

* **Source**: Data comes from external storage (S3), not local cluster storage
* **Speed**: Slower due to network transfer from S3 (minutes to hours)
* **Availability**: Works even if the original cluster no longer exists
* **New Volume**: Always creates a new volume—cannot revert an existing volume in-place

=== Basic Backup Restore Workflow

Restoring a backup is a multi-step process that involves downloading data from S3 and integrating it into your cluster.

**Step-by-Step Process:**

1.  **Access Backup Listing**:
    *  Open the Longhorn UI.
    *  Click **Backup and Restore** in the top navigation.
    *  Select the **Backups** tab.
    *  You'll see all available backups grouped by volume.

2.  **Select Backup to Restore**:
    *  Click the **Volume** tab to see volumes with available backups.
    *  Find the volume you want to restore from.
    *  Click on the volume name to see its backup history.
    *  Locate the specific backup you want to restore (check timestamp, labels, size).

3.  **Initiate Restore Operation**:
    *  Select the backup you want to restore.
    *  Click the **Restore** button.

4.  **Configure Restore Options**:
    *  Longhorn presents two restoration strategies:
+
[options="header"]
|===
|Option |Description |Use Case

|**Create a new PVC**
|Creates a new PVC with a different name (e.g., `postgres-data-restored`).
|Safe approach for testing restored data before replacing production volume

|**Replace an existing PVC**
|Deletes an existing PVC and creates a new one with the same name, containing restored data.
|Direct replacement (requires stopping the application first)

|===
+
    *  **Recommendation**: Use "Create a new PVC" to test first, then swap PVCs if satisfied.
    *  Enter the PVC name and select the target namespace.
    *  Click **OK** to start the restore.

5.  **Monitor Restore Progress**:
    *  The restore operation appears in the volume list with status indicators.
    *  **What's happening**: Longhorn downloads backup data from S3, reconstructs the volume, and makes it available as a PV/PVC.
    *  Depending on volume size and network speed, this can take from minutes to hours.
    *  Progress is shown in the UI—you can navigate away and check back later.

6.  **Verify Restore Completion**:
+
[source,bash]
----
# Check that the new PVC was created and is bound
kubectl get pvc <restored-pvc-name> -n <namespace>

# Output should show STATUS: Bound
----

IMPORTANT: **Always stop applications** using the original PVC before performing a replace-in-place restore. Restoring while the application is running can cause data corruption or restore failures.

=== Restoring PostgreSQL with ArgoCD Management

When your application is managed by ArgoCD (or any GitOps tool), you must carefully coordinate the restore process to prevent ArgoCD from interfering with manual operations. ArgoCD's automated sync and self-healing features will attempt to recreate resources you're deliberately modifying, so they must be temporarily disabled.

**Why This is Necessary:**

* ArgoCD monitors Git as the source of truth
* When it detects drift (e.g., scaled-down pods, deleted PVCs), it auto-heals by recreating resources
* During restore, you need manual control over resource lifecycles
* After restore completes, you re-enable ArgoCD to resume normal operations

==== Step-by-Step Restore Procedure

This procedure demonstrates replacing an existing PostgreSQL PVC with restored backup data:

**Step 1: Prepare Environment Variables**

[source,bash]
----
# Define application and infrastructure details
# Adjust these values to match your environment

APP_NAME=postgresql-app           # ArgoCD application name
NAMESPACE=service-foundry         # Kubernetes namespace  
STATEFULSET_NAME=postgresql       # StatefulSet name
POD_NAME=postgresql-0             # Primary pod name
PVC_NAME=data-postgresql-0        # PersistentVolumeClaim name
----

**Step 2: Disable ArgoCD Auto-Sync**

Temporarily suspend ArgoCD's automated operations:

[source,bash]
----
# Option A: Using ArgoCD CLI (if installed)
# This sets sync policy to 'none', disabling automated sync
argocd app set ${APP_NAME} --sync-policy none

# Option B: Using kubectl (works without ArgoCD CLI)
# Removes the entire syncPolicy section from the Application spec
kubectl patch application ${APP_NAME} -n argocd \
  --type merge \
  -p '{"spec":{"syncPolicy":null}}'

# Verify sync policy was removed
kubectl get application ${APP_NAME} -n argocd -o jsonpath='{.spec.syncPolicy}'
# Should output: (empty) or null
----

**Step 3: Stop the Application**

Gracefully shut down PostgreSQL to ensure no data is being written:

[source,bash]
----
# Scale StatefulSet to zero replicas
# This terminates pods and releases the PVC
kubectl scale statefulset ${STATEFULSET_NAME} --replicas=0 -n ${NAMESPACE}

# Wait for pod deletion with timeout
# This ensures the PVC is fully detached before proceeding
kubectl wait --for=delete pod/${POD_NAME} -n ${NAMESPACE} --timeout=60s

# Confirm no pods are running
kubectl get pods -n ${NAMESPACE} -l app.kubernetes.io/name=postgresql
# Expected: No resources found (or empty list)
----

**Step 4: Remove Existing PVC and PV**

Delete the current PVC to make room for the restored one:

[source,bash]
----
# Get the PV name before deleting PVC (you'll need to delete it too)
PV_NAME=$(kubectl get pvc ${PVC_NAME} -n ${NAMESPACE} -o jsonpath='{.spec.volumeName}')

echo "PVC: ${PVC_NAME}"
echo "PV:  ${PV_NAME}"

# Delete the PVC
# Warning: This removes the PVC but the underlying Longhorn volume may still exist
kubectl delete pvc ${PVC_NAME} -n ${NAMESPACE}

# Wait for PVC deletion to complete
kubectl wait --for=delete pvc/${PVC_NAME} -n ${NAMESPACE} --timeout=60s

# Delete the associated PV
# This is necessary because PVs may have retain policy and won't auto-delete
kubectl delete pv ${PV_NAME}

# Verify deletion
kubectl get pvc,pv -n ${NAMESPACE} | grep ${PVC_NAME}
# Should return no results
----

**Step 5: Restore Backup in Longhorn UI**

Perform the restore operation through the Longhorn UI:

1. Navigate to **Backup and Restore** → **Backups**
2. Under the **Volume** tab, find your PostgreSQL volume backups
3. Select the backup you want to restore (verify timestamp and labels)
4. Click **Restore**
5. In the restore dialog:
   * **PVC Name**: Enter `data-postgresql-0` (the EXACT SAME name as the deleted PVC)
   * **Namespace**: Select `service-foundry` (or your namespace)
   * **Storage Class**: Leave as `longhorn` (default)
6. Click **OK** to initiate the restore
7. **Monitor progress**: The restore shows up in the volume list with a progress indicator
8. **Wait for completion**: Status changes to "Healthy" or "Bound" when done

**Step 6: Verify Restored PVC**

Confirm the PVC was created successfully:

[source,bash]
----
# Check PVC status
kubectl get pvc ${PVC_NAME} -n ${NAMESPACE}

# Expected output:
# NAME                  STATUS   VOLUME           CAPACITY   ACCESS MODES   STORAGECLASS
# data-postgresql-0     Bound    pvc-xyz...       10Gi       RWO            longhorn

# Verify the PVC is bound to a Longhorn volume
kubectl describe pvc ${PVC_NAME} -n ${NAMESPACE}
----

**Step 7: Restart the Application**

Bring PostgreSQL back online with the restored data:

[source,bash]
----
# Scale StatefulSet back to 1 replica
# The pod mounts the restored PVC automatically
kubectl scale statefulset ${STATEFULSET_NAME} --replicas=1 -n ${NAMESPACE}

# Wait for pod to become ready
kubectl wait --for=condition=ready pod/${POD_NAME} -n ${NAMESPACE} --timeout=300s

# Check pod status
kubectl get pod ${POD_NAME} -n ${NAMESPACE}
# Expected: STATUS = Running, READY = 1/1
----

**Step 8: Re-enable ArgoCD Auto-Sync**

Restore automated GitOps management:

[source,bash]
----
# Option A: Using ArgoCD CLI
argocd app set ${APP_NAME} --sync-policy automated

# Option B: Using kubectl with full configuration
# Re-enables automated sync with prune (delete orphaned resources) 
# and selfHeal (auto-correct drift)
kubectl patch application ${APP_NAME} -n argocd \
  --type merge \
  -p '{"spec":{"syncPolicy":{"automated":{"prune":true,"selfHeal":true}}}}'

# Verify sync policy was restored
kubectl get application ${APP_NAME} -n argocd -o jsonpath='{.spec.syncPolicy}'
# Should output: {"automated":{"prune":true,"selfHeal":true}}
----


==== Data Verification After Restore

After successfully restoring and restarting your application, thorough verification is critical to ensure data integrity and completeness.

**Comprehensive Verification Checklist:**

**1. Verify Infrastructure Components:**

[source,bash]
----
# Check PVC is bound to a volume
kubectl get pvc ${PVC_NAME} -n ${NAMESPACE}
# STATUS should be "Bound"

# Verify PV exists and matches the PVC
kubectl get pv $(kubectl get pvc ${PVC_NAME} -n ${NAMESPACE} -o jsonpath='{.spec.volumeName}')

# Check PostgreSQL pod is running and ready
kubectl get pod ${POD_NAME} -n ${NAMESPACE}
# STATUS = Running, READY = 1/1

# View recent pod logs to check for startup errors
kubectl logs ${POD_NAME} -n ${NAMESPACE} --tail=50
# Look for successful startup messages, no errors
----

**2. Verify PostgreSQL Connectivity:**

[source,bash]
----
# Test database connection
kubectl exec -it ${POD_NAME} -n ${NAMESPACE} -- psql -U postgres -c "SELECT version();"

# Should return PostgreSQL version information without errors
----

**3. Verify Data Integrity:**

Connect to PostgreSQL and perform data validation:

[source,bash]
----
# Open interactive PostgreSQL session
kubectl exec -it ${POD_NAME} -n ${NAMESPACE} -- psql -U postgres
----

Inside the PostgreSQL prompt, run verification queries:

[source,sql]
----
-- List all databases (verify expected databases exist)
\l

-- Connect to your application database
\c <your-database-name>

-- List all tables (verify schema is intact)
\dt

-- Verify row counts for critical tables
SELECT count(*) FROM users_for_backup_testing;
-- Compare against expected counts from before the incident

-- Check for recent data (if applicable)
SELECT * FROM users_for_backup_testing ORDER BY id DESC LIMIT 10;

-- Verify specific records that should exist in the backup
SELECT * FROM users_for_backup_testing WHERE name IN ('kim', 'lee');
-- Should return the expected test data

-- Exit PostgreSQL
\q
----

**4. Verify Application Functionality:**

[source,bash]
----
# If your application has a health check endpoint
kubectl exec ${POD_NAME} -n ${NAMESPACE} -- curl http://localhost:5432

# Or test application-level queries
# (connect through your application and verify it can read/write data)
----

**5. Check Longhorn Volume Metadata:**

Verify the volume was properly restored from backup:

1. Open Longhorn UI
2. Navigate to **Volume** page
3. Find the restored volume (named `pvc-<uid>`)
4. Click to open details
5. Go to **Snapshots and Backups** tab
6. Check the **Restored From** timestamp or metadata
7. Verify it matches the backup you intended to restore

TIP: **Best Practice**: Always test restores in a non-production environment first. Consider keeping the old PVC for a few days as insurance before permanently deleting it. Create a test namespace, restore there, verify fully, then restore to production.

== Deleting Snapshots and Backups

Regular cleanup of old snapshots and backups is essential for managing storage costs and maintaining system performance. However, deletion must be done carefully to avoid accidentally removing data you might need for recovery.

=== Deleting Snapshots

Snapshots consume space within your Longhorn cluster. Old or unnecessary snapshots should be removed to free up storage, but be cautious—once deleted, they cannot be recovered.

**When to Delete Snapshots:**

* After creating a successful backup (the backup preserves the snapshot data externally)
* When snapshots exceed your retention policy (e.g., keeping only the last 7 days)
* After confirming a risky operation succeeded and rollback is no longer needed
* When cluster storage is running low and snapshots are consuming significant space

WARNING: **Do not delete snapshots that haven't been backed up** if you need long-term retention. Snapshots are your only local recovery point—once deleted, you can only restore from external backups.

**Deletion Procedure:**

1.  **Navigate to Volume Page**:
    *  Open the Longhorn UI.
    *  Click **Volume** in the top navigation.

2.  **Select Source Volume**:
    *  Find and click the volume containing the snapshot you want to delete.

3.  **Access Snapshot List**:
    *  Scroll down to the **Snapshots and Backups** section.
    *  Review the list of snapshots with timestamps, sizes, and labels.

4.  **Delete Snapshot**:
    *  Click on the snapshot name to open the action menu.
    *  Select **Delete** from the dropdown.
    *  **Confirm the deletion** in the dialog (read the warning carefully).
    *  The snapshot is immediately deleted and storage is reclaimed.

5.  **Verify Deletion**:
    *  The snapshot should disappear from the list.
    *  Check the volume's **Actual Size** to see if storage was freed (may take a moment to update).

**Automated Cleanup:**

Longhorn supports automatic snapshot cleanup through recurring job retention policies. When you configure a recurring backup job with a retention count (e.g., retain 7 backups), Longhorn automatically deletes older snapshots and backups when new ones are created.

TIP: Before deleting a snapshot, verify that a corresponding backup exists in S3 (if you need long-term retention). Check the **Backup** tab to confirm.

=== Deleting Backups

Backups stored in S3 (or other external targets) consume cloud storage and incur costs. Removing old backups reduces these costs but should align with your retention policy and compliance requirements.

**When to Delete Backups:**

* Backups older than your retention policy (e.g., keeping only 30 days)
* Test backups created during development or validation
* Backups from decommissioned volumes or applications
* When migrating to a new backup strategy or storage location

**Important Considerations:**

* Backups may be required for compliance (GDPR, HIPAA, SOX, etc.)
* Coordinate with legal/compliance teams before deleting long-term backups
* Backups are your disaster recovery insurance—excessive deletion increases risk

**Deletion Procedure:**

1.  **Navigate to Backup List**:
    *  Open the Longhorn UI.
    *  Click **Backup and Restore** → **Backups**.

2.  **Find Target Backup**:
    *  Click the **Volume** tab to see backups grouped by volume.
    *  Select the volume containing the backup you want to delete.
    *  Click on the volume to see its backup history.

3.  **Select Backup to Delete**:
    *  Review the backup list (check timestamps, size, labels).
    *  Select the specific backup you want to delete.

4.  **Initiate Deletion**:
    *  Click the **Delete** button.
    *  A confirmation dialog appears with two options:
+
[options="header"]
|===
|Option |Description |Impact

|**Delete the backup only**
|Removes only this specific backup from S3, leaving the Longhorn volume intact.
|Recommended for most cases (frees S3 storage, keeps active volumes)

|**Delete the backup and the volume**
|Removes the backup from S3 **AND** deletes the Longhorn volume in the cluster.
|**DANGEROUS**: Only use when decommissioning entire volumes

|===
+
    *  **Choose carefully**: Most users want "Delete the backup only."
    *  Click **OK** to confirm.

5.  **Verify Deletion**:
    *  The backup should disappear from the backup list.
    *  Optionally, check your S3 bucket to confirm the backup files were removed (may take a few minutes).

**Bulk Deletion (Advanced):**

For deleting many old backups, consider using AWS CLI:

[source,bash]
----
# List backups older than 30 days in your S3 bucket
aws s3 ls s3://your-longhorn-backups/backups/ --recursive \
  | awk '{if ($1 < "2024-01-01") print $4}'

# Carefully review the list, then delete (use with caution!)
# This is advanced usage - test in non-production first
----

WARNING: **Backups are immutable once created**. There is no "undo" for backup deletion. Always verify you're deleting the correct backup and that you have other recovery options if needed.

TIP: Implement a backup lifecycle policy: Short retention for frequent backups (e.g., 7 days daily), longer retention for weekly (30 days), and potentially permanent retention for monthly compliance backups.




== Conclusion

Throughout this comprehensive guide, you've learned how to implement a complete data protection strategy for Longhorn persistent storage in Kubernetes. Let's recap the key concepts and best practices:

**What We've Covered:**

1. **Data Protection Layers**: Understanding how HA, snapshots, and backups work together to provide defense-in-depth:
   * High Availability protects against infrastructure failures
   * Snapshots enable fast operational recovery
   * Backups provide true disaster recovery and long-term retention

2. **AWS S3 Integration**: Configuring secure, off-cluster backup storage:
   * Creating S3 buckets with appropriate security settings
   * Implementing least-privilege IAM policies
   * Managing credentials securely using Kubernetes Secrets

3. **Operational Procedures**: Practical workflows for backup and restore operations:
   * Manual and automated backup creation
   * Snapshot-based quick recovery
   * Off-cluster backup restoration for disaster recovery
   * ArgoCD integration for GitOps-managed applications

4. **Maintenance Operations**: Managing the lifecycle of backups and snapshots:
   * Implementing retention policies
   * Safely deleting obsolete backups and snapshots
   * Balancing storage costs against recovery needs

**Key Takeaways:**

[quote]
____
A production-grade backup strategy requires **multiple layers of protection**. High availability keeps your services running, snapshots provide fast operational recovery, and off-cluster backups ensure you can survive catastrophic failures. All three are essential—not optional.
____

**Next Steps:**

1.  **Test Your Restore Procedures**: The best backup strategy is worthless if restoration doesn't work. Perform regular restore drills in non-production environments.

2.  **Automate Backup Schedules**: Configure recurring backup jobs for all critical volumes with appropriate retention policies.

3.  **Implement Monitoring**: Set up alerts for backup failures using Prometheus and Alertmanager to ensure backups complete successfully.

4.  **Document Your RPO/RTO**: Define Recovery Point Objectives (how much data loss is acceptable) and Recovery Time Objectives (how quickly you need to recover) for each application.

5.  **Consider IRSA for Production**: For production AWS deployments, migrate from static IAM user credentials to IAM Roles for Service Accounts (IRSA) for improved security.

6.  **Review Compliance Requirements**: Ensure your backup retention policies meet regulatory requirements (GDPR, HIPAA, SOX, etc.).

**Remember:**

* Backups are insurance—you hope never to need them, but when you do, they're invaluable
* Regular testing is the only way to ensure backups actually work when needed
* Balance retention policies against storage costs and compliance requirements
* Document your procedures so anyone on your team can perform emergency restores

By implementing the practices outlined in this guide, you've established a robust foundation for data protection in your Kubernetes environment. Your persistent data is now protected against infrastructure failures, operational mistakes, and catastrophic disasters.

== References and Further Reading

*   **Official Longhorn Documentation**:
    **  https://longhorn.io/docs/latest/snapshots-and-backups/[Longhorn Snapshots and Backups]
    **  https://longhorn.io/docs/latest/backup-and-restore/[Backup and Restore]
    **  https://longhorn.io/docs/latest/best-practices/[Best Practices]

*   **AWS S3 Security**:
    **  https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-best-practices.html[S3 Security Best Practices]
    **  https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html[IAM Best Practices]

*   **Kubernetes Storage**:
    **  https://kubernetes.io/docs/concepts/storage/persistent-volumes/[Persistent Volumes]
    **  https://kubernetes.io/docs/concepts/storage/storage-classes/[Storage Classes]

*   **IRSA (IAM Roles for Service Accounts)**:
    **  https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html[EKS IRSA Documentation]

For questions, issues, or contributions, visit the Longhorn community:

*   GitHub: https://github.com/longhorn/longhorn
*   Slack: https://cloud-native.slack.com/ (#longhorn channel)
*   Forums: https://forums.rancher.com/c/longhorn