= Longhorn: How to Use Shared ReadWriteMany Volumes for Stateful Applications

:imagesdir: images

[.img-wide]
image::stateful-app-with-rwx-volume.png[]


YouTube Video Tutorial: https://youtu.be/gF8f7235mE4

== Introduction

Longhorn is a cloud-native, distributed block storage system designed specifically for Kubernetes. As an open-source, CNCF (Cloud Native Computing Foundation) project, Longhorn provides enterprise-grade persistent storage capabilities with high availability, disaster recovery, and volume management features that integrate seamlessly with Kubernetes infrastructure.

=== What is ReadWriteMany (RWX)?

In Kubernetes, persistent volumes support different access modes that define how they can be mounted and accessed:

* **ReadWriteOnce (RWO)**: The volume can be mounted as read-write by a single node. This is the most common access mode and is suitable for most stateful applications like databases.
* **ReadOnlyMany (ROX)**: The volume can be mounted as read-only by multiple nodes simultaneously.
* **ReadWriteMany (RWX)**: The volume can be mounted as read-write by multiple nodes simultaneously. This is essential for workloads that require shared storage across multiple pods.

=== Why RWX Volumes Matter

In our previous article, we installed Longhorn on AWS EKS, which created a default `longhorn` StorageClass with ReadWriteOnce (RWO) access mode. This configuration worked perfectly for stateful applications like PostgreSQL and Redis, where a single pod manages the database instance and requires exclusive write access to the storage.

However, many modern distributed applications and data processing frameworks require shared storage that can be accessed concurrently by multiple pods. Common use cases include:

* **Apache Airflow**: Multiple scheduler and worker pods need to write logs and share metadata in a common directory
* **Machine Learning platforms**: Training jobs running across multiple nodes need to access shared datasets and write checkpoints
* **Content Management Systems**: Multiple application servers need to access and modify shared media files
* **Shared logging infrastructure**: Applications across different pods writing to centralized log directories

This article demonstrates how to configure and use Longhorn's ReadWriteMany (RWX) volumes for stateful applications that require shared, concurrent write access to persistent storage.

=== What We'll Build

We will create a practical demonstration using a custom Go application that collects system metrics from host nodes and writes them to log files. The application will be deployed as a DaemonSet, ensuring it runs on every node in the cluster. By mounting a shared RWX volume, we'll demonstrate that:

1. Multiple pods across different nodes can write to the same persistent volume simultaneously
2. Data written by any pod is immediately visible to all other pods sharing the volume
3. The shared storage remains consistent and accessible even as pods scale up or down

NOTE: The example application is designed for demonstration purposes to illustrate RWX volume behavior. In production environments, you would typically use more sophisticated logging solutions like the ELK stack, Loki, or other centralized logging platforms.


== Prerequisites

Before proceeding with this tutorial, ensure you have the following components installed and configured:

* **Longhorn Storage System**: A fully operational Longhorn installation on your Kubernetes cluster with the default StorageClass configured
* **kubectl**: The Kubernetes command-line tool (version 1.20 or later recommended) configured to communicate with your cluster
* **Kubernetes Cluster**: A running Kubernetes cluster (version 1.20 or later) with at least 3 worker nodes for demonstrating distributed storage capabilities
* **Helm**: Package manager for Kubernetes (version 3.x or later) for installing Helm charts that require RWX storage

=== Installation Reference

If you haven't installed Longhorn yet, please refer to our comprehensive guide:

- link:https://medium.com/@nsalexamy/longhorn-highly-available-distributed-block-storage-on-aws-eks-68aebce503ba[Longhorn: Highly Available, Distributed Block Storage on AWS EKS]

This guide covers the complete Longhorn installation process, including:

* Infrastructure setup on AWS EKS
* Longhorn system components deployment
* Storage class configuration
* Volume provisioning and management
* Basic troubleshooting procedures

== Sample Application

=== Application Overview

For this demonstration, we've developed a custom Go application called `custom-node-exporter` that serves as a practical example of using RWX volumes. This application performs the following operations:

1. **Metrics Collection**: Gathers system-level metrics from the host node, including CPU usage, memory consumption, disk I/O statistics, and network traffic
2. **Data Logging**: Writes these metrics as structured JSON records to log files stored on the shared RWX volume
3. **Scheduled Execution**: Collects and writes metrics every minute, creating a continuous stream of operational data
4. **Node Identification**: Each log record includes the hostname, allowing us to track which node generated each entry

=== Deployment Strategy: DaemonSet

The application is deployed as a Kubernetes DaemonSet, which ensures that exactly one pod runs on each node in the cluster. This deployment pattern is ideal for our demonstration because:

* **Node Coverage**: As nodes are added to or removed from the cluster, the DaemonSet automatically manages pod distribution
* **Host Access**: Each pod can collect authentic system metrics from its respective host node
* **Concurrent Writers**: Multiple pods simultaneously write to the shared volume, demonstrating true RWX behavior
* **Data Verification**: We can easily verify that logs from all nodes appear in the shared directory

=== Expected Output

By the end of this tutorial, you will observe log files from all cluster nodes coexisting in the same shared directory. Each log file will contain timestamped system metrics in JSON format, demonstrating successful concurrent write operations across multiple nodes.

Here is a sample log record that the application generates:

.Sample log record showing system metrics in JSON format
[.img-wide]
image::sample-log-record.png[]

Each log entry contains comprehensive system metrics including CPU utilization, memory statistics, disk I/O counters, network traffic, and process information, all timestamped and associated with the specific node that generated the data.

== Implementation Steps

To implement shared ReadWriteMany storage with Longhorn, we'll follow a systematic four-step approach:

. **Configure the StorageClass**: Create a dedicated Longhorn StorageClass with RWX-specific parameters, including the critical `migratable: false` setting required for shared volume support
. **Provision the Persistent Volume**: Create a PersistentVolumeClaim (PVC) that requests ReadWriteMany access mode, which triggers Longhorn to deploy the NFS-based share-manager infrastructure
. **Deploy the Application**: Deploy the custom node-exporter application as a DaemonSet to ensure it runs on every cluster node, with each pod mounting the shared RWX volume
. **Verify Shared Access**: Confirm that multiple pods can simultaneously write to the shared volume and that data is accessible across all nodes using both debug pods and direct share-manager inspection


=== Step 1: Create Longhorn StorageClass for RWX Volumes

The first step is to create a dedicated StorageClass configured specifically for ReadWriteMany access. While Longhorn's default StorageClass handles RWO volumes, RWX volumes require special configuration due to their different architectural requirements.

[source, yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-rwx  # <1>
provisioner: driver.longhorn.io  # <2>
allowVolumeExpansion: true  # <3>
reclaimPolicy: Delete  # <4>
volumeBindingMode: Immediate  # <5>
parameters:
  numberOfReplicas: "3"  # <6>
  staleReplicaTimeout: "30"  # <7>
  fromBackup: ""  # <8>
  migratable: "false"  # <9>
----

<1> **StorageClass Name**: `longhorn-rwx` clearly identifies this class as supporting ReadWriteMany access mode, distinguishing it from the default RWO class
<2> **Provisioner**: `driver.longhorn.io` is the CSI (Container Storage Interface) driver that Longhorn uses to dynamically provision volumes
<3> **Volume Expansion**: Allows volumes to be expanded after creation without downtime, useful for growing log directories or shared data stores
<4> **Reclaim Policy**: `Delete` ensures that when a PVC is deleted, the underlying PV and its data are also removed, preventing orphaned storage resources
<5> **Volume Binding**: `Immediate` binding provisions the volume as soon as the PVC is created, rather than waiting for a pod to use it
<6> **Number of Replicas**: Maintains three copies of the data across different nodes for high availability and fault tolerance
<7> **Stale Replica Timeout**: Defines how long (in minutes) Longhorn waits before considering a replica stale and rebuilding it on another node
<8> **From Backup**: Leave empty for new volumes; this parameter is used when creating volumes from existing Longhorn backups
<9> **Migratable Flag**: **CRITICAL** - Must be set to `"false"` for RWX volumes due to how Longhorn implements shared storage via NFS

==== Understanding the Migratable Constraint

RWX volumes in Longhorn work by creating a share-manager pod that exports the volume via NFS (Network File System). This architecture requires the volume to remain attached to a specific node where the share-manager runs, making migration incompatible with the RWX implementation.

According to the official Longhorn documentation:

[quote, https://longhorn.io/docs/1.10.1/nodes-and-volumes/volumes/rwx-volumes/, ReadWriteMany Volumes]
____
An RWX volume must have the access mode set to ReadWriteMany and the "migratable" flag disabled (parameters.migratable: false). This is because RWX volumes are implemented using a share-manager pod that exports the volume via NFS, and this pod must remain on a specific node.
____

==== Apply the StorageClass

Create the StorageClass in your cluster:

.Create the storage class
[source,shell]
----
$ kubectl apply -f longhorn-rwx-sc.yaml
storageclass.storage.k8s.io/longhorn-rwx created
----

==== Verify StorageClass Creation

Confirm that the new StorageClass is available:

[source,shell]
----
$ kubectl get storageclasses
NAME                 PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
gp2                  kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  2d3h
longhorn (default)   driver.longhorn.io      Delete          Immediate              true                   2d1h
longhorn-rwx         driver.longhorn.io      Delete          Immediate              true                   62m
longhorn-static      driver.longhorn.io      Delete          Immediate              true                   2d1h
----

You should see the `longhorn-rwx` StorageClass in the list with the Longhorn provisioner.


=== Step 2: Create a PersistentVolumeClaim (PVC)

Now we'll create a PersistentVolumeClaim that requests storage with ReadWriteMany access mode. This PVC will be used by our custom-node-exporter application to store logs that need to be accessible from all nodes.

[source, yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: custom-node-exporter-logs-pvc  # <1>
  namespace: service-foundry  # <2>
spec:
  accessModes:
    - ReadWriteMany  # <3>
  storageClassName: longhorn-rwx  # <4>
  resources:
    requests:
      storage: 1Gi  # <5>
----

<1> **PVC Name**: Descriptive name indicating this volume stores logs for the custom-node-exporter application
<2> **Namespace**: The application namespace where the custom-node-exporter DaemonSet will run
<3> **Access Mode**: `ReadWriteMany` allows the volume to be mounted with read-write permissions by pods on multiple nodes simultaneously
<4> **Storage Class**: References the `longhorn-rwx` StorageClass we created in the previous step
<5> **Storage Request**: Requests 1GiB of storage capacity; this can be adjusted based on your expected log volume

==== Understanding RWX Access Mode

When you specify `ReadWriteMany`, Kubernetes instructs Longhorn to:

1. Create a backing Longhorn volume with the requested capacity
2. Deploy a share-manager pod that mounts the Longhorn volume
3. Configure the share-manager to export the volume via NFS
4. Allow multiple pods across different nodes to mount this NFS export simultaneously

This differs from `ReadWriteOnce`, where the volume is directly attached to a single node via iSCSI protocol.

==== Apply the PVC

Create the PersistentVolumeClaim:

[source,shell]
----
$ kubectl apply -f pvc.yaml
persistentvolumeclaim/custom-node-exporter-logs-pvc created
----

==== Verify PVC Status

Check that the PVC has been successfully bound to a PersistentVolume:

[source,shell]
----
$ kubectl get pvc -n service-foundry

NAME                            STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
custom-node-exporter-logs-pvc   Bound    pvc-d5537143-35b9-4669-bb95-50fa8b7a6f19   1Gi        RWX            longhorn-rwx   <unset>                 52m
----

**Key observations:**

* **STATUS**: Should be `Bound`, indicating Longhorn successfully provisioned the volume
* **ACCESS MODES**: Displays `RWX`, confirming ReadWriteMany access
* **STORAGECLASS**: Shows `longhorn-rwx`, our custom StorageClass
* **VOLUME**: The dynamically provisioned PersistentVolume name (auto-generated by Longhorn)

If the PVC remains in `Pending` status, check the Longhorn UI or logs for provisioning errors.


=== Step 3: Deploy the Application as a DaemonSet

We'll now deploy the custom-node-exporter application as a DaemonSet, which will run a pod on every node in the cluster. Each pod will mount the shared RWX volume and write logs to it, demonstrating concurrent write access.

[source, yaml]
----
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: custom-node-exporter  # <1>
  namespace: service-foundry
  labels:
    app: custom-node-exporter
spec:
  selector:
    matchLabels:
      app: custom-node-exporter  # <2>
  template:
    metadata:
      labels:
        app: custom-node-exporter
    spec:
      hostNetwork: true  # <3>
      hostPID: true  # <4>
      containers:
      - name: node-exporter
        image: credemol/custom-node-exporter:0.5.0  # <5>
        imagePullPolicy: Always  # <6>
        ports:
        - containerPort: 9100  # <7>
          hostPort: 9100
          name: metrics
        env:
        - name: LOG_DIR  # <8>
          value: /var/log/custom-node-exporter
        # Additional environment variables for metric collection intervals,
        # log rotation settings, and other configuration options can be added here

        volumeMounts:
        - name: logs  # <9>
          mountPath: /var/log/custom-node-exporter
        # Additional volume mounts for accessing host system information
        # could be added here (e.g., /proc, /sys for system metrics)

      volumes:
      - name: logs  # <10>
        persistentVolumeClaim:
          claimName: custom-node-exporter-logs-pvc
      # Additional volumes for host filesystem access can be added here
----

<1> **DaemonSet Name**: Identifies this workload as the custom-node-exporter
<2> **Label Selector**: Ensures the DaemonSet manages pods with matching labels; this is required for DaemonSet pod management
<3> **Host Network**: Uses the host's network namespace, allowing the application to see the actual node's network interfaces and collect authentic network metrics
<4> **Host PID**: Accesses the host's process namespace, enabling the application to monitor all processes running on the node, not just container processes
<5> **Container Image**: The custom Go application packaged as a Docker image; version 0.5.0 includes the metrics collection and logging functionality
<6> **Image Pull Policy**: `Always` ensures the latest image is pulled on each pod creation, useful during development and testing
<7> **Port Configuration**: Exposes port 9100 on both the container and host, following the Prometheus node_exporter convention for metrics endpoints (though this is supplementary to our log-based demonstration)
<8> **Environment Variable**: Configures the application to write logs to `/var/log/custom-node-exporter`, which maps to our RWX volume mount
<9> **Volume Mount**: Mounts the `logs` volume at `/var/log/custom-node-exporter` inside the container, making the shared storage accessible to the application
<10> **Volume Definition**: References the PVC we created earlier, which provides the shared RWX storage

==== How DaemonSets Ensure Node Coverage

The DaemonSet controller automatically:

* Creates one pod on each node when the DaemonSet is deployed
* Creates a new pod when a node is added to the cluster
* Removes pods when nodes are removed from the cluster
* Recreates pods if they fail or are deleted

This makes DaemonSets ideal for node-level services like log collectors, monitoring agents, and storage daemons.

==== Apply the DaemonSet

Deploy the application:

[source,shell]
----
$ kubectl apply -f daemonset.yaml
daemonset.apps/custom-node-exporter created
----

==== Verify DaemonSet Deployment

Check that pods are running on all nodes:

[source,shell]
----
$ kubectl get pods -n service-foundry -l app=custom-node-exporter -o wide

NAME                       READY   STATUS    RESTARTS   AGE   IP              NODE
custom-node-exporter-abc   1/1     Running   0          2m    192.168.25.18   ip-192-168-25-18.ca-central-1.compute.internal
custom-node-exporter-def   1/1     Running   0          2m    192.168.50.87   ip-192-168-50-87.ca-central-1.compute.internal
custom-node-exporter-ghi   1/1     Running   0          2m    192.168.58.1    ip-192-168-58-1.ca-central-1.compute.internal
----

You should see one pod per node in your cluster, all in the `Running` state. The `NODE` column shows the distribution across different nodes.

== Verification: Confirming Shared Volume Access

Now that our application is running and writing logs to the shared RWX volume, we need to verify that the volume is indeed accessible from multiple pods simultaneously and that data written by different pods coexists in the same storage.

We'll use two different approaches to verify this behavior:

1. **Debug Pod**: Create a separate pod that mounts the same PVC to inspect the shared data
2. **Share-Manager Pod**: Directly access Longhorn's share-manager pod, which manages the NFS export

=== Method 1: Using a Debug Pod

Creating a dedicated debug pod provides a clean, isolated environment for inspecting the shared volume without interfering with the running application.

==== Create the Debug Pod

Here's the manifest for a simple debug pod:

.longhorn-rwx-debug-pod.yaml
[source, yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: longhorn-rwx-debug  # <1>
  namespace: service-foundry
spec:
  containers:
  - name: longhorn-rwx-debug
    image: busybox  # <2>
    command: ["sh", "-c", "sleep 36000"]  # <3>
  restartPolicy: Never  # <4>
  volumeMounts:
  - mountPath: /data  # <5>
    name: vol
  volumes:
  - name: vol  # <6>
    persistentVolumeClaim:
      claimName: custom-node-exporter-logs-pvc
----

<1> **Pod Name**: Identifies this as a debugging tool for inspecting RWX volume contents
<2> **Busybox Image**: Lightweight Linux container with essential shell utilities, perfect for debugging without unnecessary overhead
<3> **Sleep Command**: Keeps the container running for 10 hours (36000 seconds), giving you ample time to exec into it and explore the volume
<4> **Restart Policy**: `Never` prevents automatic restart if the container exits, appropriate for debugging sessions
<5> **Mount Path**: Mounts the volume at `/data` inside the container for easy access
<6> **Volume Reference**: Uses the **same PVC** that the DaemonSet pods are using, demonstrating that multiple pods can mount the same RWX volume

==== Deploy and Access the Debug Pod

Create the pod and exec into it:

[source,shell]
----
$ kubectl apply -f longhorn-rwx-debug-pod.yaml
pod/longhorn-rwx-debug created

$ POD_NAME=longhorn-rwx-debug

$ kubectl exec -it -n service-foundry $POD_NAME -- /bin/sh
----

==== Inspect the Shared Volume

Once inside the debug pod, list the contents of the mounted volume:

[source,shell]
----
# Inside the debug pod shell
$ ls -l /data

ip-192-168-25-18.ca-central-1.compute.internal-log-scheduler-2026-01-27.log
ip-192-168-25-18.ca-central-1.compute.internal-log-scheduler-2026-01-28.log
ip-192-168-50-87.ca-central-1.compute.internal-log-scheduler-2026-01-27.log
ip-192-168-50-87.ca-central-1.compute.internal-log-scheduler-2026-01-28.log
ip-192-168-58-1.ca-central-1.compute.internal-log-scheduler-2026-01-27.log
ip-192-168-58-1.ca-central-1.compute.internal-log-scheduler-2026-01-28.log
lost+found
----

==== Understanding the Results

**Log File Naming Convention:** `<node-hostname>-log-scheduler-<date>.log`

Each filename includes:

* **Node hostname**: Identifies which node's pod created the file
* **Date**: Logs are rotated daily, creating new files each day

**Key Observations:**

1. **Multiple Writers**: You see log files from multiple different nodes (identified by different IP addresses in the hostnames), proving that different pods on different nodes are all writing to the same shared storage
2. **Data Persistence**: Files created by the DaemonSet pods are visible from a completely different pod (the debug pod)
3. **Concurrent Access**: All pods are simultaneously writing to their respective log files without conflicts

The `lost+found` directory is an ext4 filesystem artifact that appears in the root of the mounted volume and can be safely ignored.

==== View Log Contents

You can examine the actual log data:

[source,shell]
----
$ cat /data/ip-192-168-25-18.ca-central-1.compute.internal-log-scheduler-2026-01-27.log | head -5
----

This will show the JSON-formatted system metrics that the application is collecting.


=== Method 2: Using the Share-Manager Pod

For a deeper understanding of how Longhorn implements RWX volumes, we can access the share-manager pod directly. This method reveals the underlying NFS architecture.

==== What is the Share-Manager Pod?

The share-manager pod is a critical component of Longhorn's RWX implementation:

* **Automatic Creation**: Longhorn automatically creates this pod when your first RWX PVC is provisioned
* **NFS Server Role**: The pod runs an NFS (Network File System) server that exports the Longhorn volume
* **Location**: Runs in the `longhorn-system` namespace
* **Single Instance**: One share-manager pod can handle multiple RWX volumes
* **Volume Mounting**: The pod mounts the actual Longhorn block storage volume and shares it via NFS to all pods that need RWX access

This architecture allows multiple pods on different nodes to mount the same volume concurrently, as NFS is designed for shared, network-based file access.

==== Access the Share-Manager Pod

Find and access the share-manager pod:

[source,shell]
----
# Automatically get the share-manager pod name using label selector
$ POD_NAME=$(kubectl get pod -l longhorn.io/component=share-manager -n longhorn-system -o jsonpath='{.items[0].metadata.name}')

# Exec into the pod
$ kubectl exec -it -n longhorn-system $POD_NAME -- /bin/sh
----

==== Explore the Volume Export Directory

Inside the share-manager pod, volumes are exported from the `/export` directory:

[source,shell]
----
# Inside the share-manager pod shell

# List exported volumes
$ ls /export

pvc-d5537143-35b9-4669-bb95-50fa8b7a6f19

# List contents of our specific volume
$ ls /export/pvc-d5537143-35b9-4669-bb95-50fa8b7a6f19

ip-192-168-25-18.ca-central-1.compute.internal-log-scheduler-2026-01-27.log
ip-192-168-25-18.ca-central-1.compute.internal-log-scheduler-2026-01-28.log
ip-192-168-50-87.ca-central-1.compute.internal-log-scheduler-2026-01-27.log
ip-192-168-50-87.ca-central-1.compute.internal-log-scheduler-2026-01-28.log
ip-192-168-58-1.ca-central-1.compute.internal-log-scheduler-2026-01-27.log
ip-192-168-58-1.ca-central-1.compute.internal-log-scheduler-2026-01-28.log
lost+found
----

==== Understanding the Architecture

**Directory Structure:**

* `/export`: The root directory for NFS exports
* `pvc-<UUID>`: Subdirectory for each RWX PVC, matching the PV name
* Log files: The actual application data

**Data Flow:**

1. Application pods write to `/var/log/custom-node-exporter` (their mount path)
2. This maps to an NFS mount provided by the share-manager pod
3. The share-manager pod stores the actual data on the Longhorn volume
4. Longhorn replicates this data across multiple nodes for fault tolerance

**What This Proves:**

* All data written by the DaemonSet pods is visible in the share-manager's export directory
* The same files are accessible whether you view them from the debug pod, the application pods, or the share-manager pod
* This confirms that Longhorn is correctly implementing shared storage via NFS


== Conclusion

In this comprehensive guide, we successfully demonstrated how to configure and use Longhorn's ReadWriteMany (RWX) volumes for stateful applications that require concurrent write access from multiple pods across different nodes.

=== What We Accomplished

1. **StorageClass Configuration**: Created a dedicated `longhorn-rwx` StorageClass with the required parameters for RWX access, including the critical `migratable: false` setting
2. **PVC Provisioning**: Provisioned a PersistentVolumeClaim with `ReadWriteMany` access mode, which Longhorn fulfilled by automatically deploying NFS-based shared storage
3. **Application Deployment**: Deployed a custom-node-exporter application as a DaemonSet, ensuring pods on every node could write to the shared volume simultaneously
4. **Verification**: Confirmed shared access using both a debug pod and direct inspection of the share-manager pod, proving that data from all nodes coexists in the same storage

=== Key Takeaways

**Architectural Understanding:**

* Longhorn implements RWX volumes by creating a share-manager pod that exports the Longhorn volume via NFS
* This architecture differs from RWO volumes, which use direct iSCSI attachment
* The NFS layer enables multiple pods on different nodes to mount the same volume concurrently with read-write access

**Configuration Essentials:**

* Always set `migratable: false` for RWX StorageClasses
* Use descriptive naming conventions for StorageClasses and PVCs to distinguish RWX from RWO resources
* Consider replica counts based on your high-availability requirements

**Use Case Applicability:**

* RWX volumes are ideal for applications requiring shared logs, shared configuration files, or collaborative workspaces
* Common scenarios include Apache Airflow, Jupyter notebooks, CI/CD systems, and content management platforms
* Not suitable for databases or applications requiring exclusive write access (use RWO for those)

=== Production Considerations

While this demonstration used a simple logging application, real-world production deployments should consider:

* **Performance**: NFS adds network overhead compared to direct block storage; benchmark your workloads
* **Monitoring**: Track NFS performance metrics and share-manager pod health
* **Capacity Planning**: Monitor volume usage and configure alerts for capacity thresholds
* **Backup Strategy**: Implement Longhorn snapshots and backups for RWX volumes
* **Access Control**: Use NetworkPolicies and RBAC to control which workloads can access shared volumes

=== Next Steps

To further explore Longhorn's capabilities:

* Experiment with volume snapshots and backups for disaster recovery
* Configure recurring backup jobs for critical shared data
* Explore Longhorn's disaster recovery features for cross-cluster replication
* Implement monitoring and alerting for storage health and capacity
* Test volume expansion to handle growing data requirements

By mastering RWX volumes, you unlock the ability to deploy a wider range of cloud-native applications on Kubernetes with Longhorn, making your infrastructure more versatile and capable of supporting modern distributed workloads.




