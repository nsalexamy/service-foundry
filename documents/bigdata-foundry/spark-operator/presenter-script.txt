Title Slide – Running Spark Applications on Kubernetes with Spark Operator and Airflow 3.0

Welcome to this guide on running Apache Spark applications on Kubernetes using the Spark Operator, and orchestrating them with Apache Airflow 3 point zero. In this video, we’ll go step by step through installing the Spark Operator, configuring the cluster, submitting Spark applications declaratively, and integrating everything with Airflow using the Spark Kubernetes Operator.

⸻

What is the Spark Operator?

Apache Spark is a powerful distributed computing engine widely used for big data processing. The Spark Operator makes it easier to deploy and manage Spark applications on Kubernetes. It introduces custom resource definitions, or CRDs, so you can define Spark applications as native Kubernetes resources. This allows you to create, monitor, and delete Spark jobs using familiar Kubernetes tools like kubectl.

⸻

Installing the Spark Operator with Helm

To get started, we’ll install the Spark Operator using Helm. First, we add the Helm repository provided by Kubeflow. Then we update the repo and confirm it’s been added successfully.

Next, we download the chart and export the default values file. As of October 2025, the latest version is 2 point 3 point 0.

We customize the values file to specify which namespaces the Spark Operator can manage. In this case, we allow spark-jobs, airflow, and default.

If those namespaces don’t already exist, we need to create them using kubectl. You can use the get or create command to check and create them as needed.

Once everything is set, we install the Spark Operator into the spark-operator namespace using Helm and our custom values file.

⸻

Uninstalling the Spark Operator

If you ever need to uninstall the operator, simply run the Helm uninstall command, specifying the release name and namespace.

⸻

Creating a Sample Spark Application

Now let’s look at a sample Spark application. The Spark Operator GitHub repository includes several examples. We’ll use the spark-pi YAML file, which calculates the value of Pi using Spark.

Update the namespace field to spark-jobs, then apply the file using kubectl.

To verify that it’s running, check the pods in the spark-jobs namespace. You can also inspect the SparkApplication resource and view logs from the driver pod to confirm the job executed correctly.

⸻

Integrating Spark Operator with Airflow 3 point 0

To orchestrate Spark jobs with Airflow, first make sure Apache Airflow 3 is installed on Kubernetes. You can follow our written guide or watch the YouTube videos linked in this section to get it set up using Helm or GitOps.

⸻

Organizing Spark Applications in the Airflow Git Repository

To integrate Spark with Airflow, we need to store the Spark application YAML files inside the dags folder in the Git repository. Create a directory called spark-apps, and place the Spark YAML files there.

For example, your repository might include the spark-pi YAML, along with DAG files like hello_world_dag dot py and spark-py-example dot py.

Note that the spark-apps folder must be placed under the dags directory. Otherwise, Airflow will not be able to access the files.

⸻

Defining the DAG with SparkKubernetesOperator

Let’s take a look at the DAG file. The spark-py-example DAG defines a simple three-step workflow. It includes a start task, a SparkKubernetesOperator task that submits the job, and a done task.

The operator references the spark-pi YAML file located inside the spark-apps directory. It also uses the Kubernetes default connection ID and specifies the target namespace.

Make sure the namespace defined in the Spark YAML matches the one used in the DAG. If the DAG says “airflow,” the YAML file must also use the airflow namespace.

⸻

Reviewing the spark-pi YAML

This is the spark-pi YAML file. It defines a SparkApplication resource, including image details, driver and executor configuration, memory limits, and security context. The configuration follows best practices for running as a non-root user with limited privileges.

Again, double-check that the namespace in this YAML matches the one defined in your DAG file.

⸻

Setting Up RBAC for Airflow

To allow Airflow to create SparkApplication resources, we need to configure RBAC. Create a Role and RoleBinding that grants the airflow-worker service account permission to manage Spark applications in the airflow namespace.

Apply the RBAC manifest using kubectl before running your DAG.

⸻

Running the DAG and Monitoring Results

Now that everything is in place, go to the Airflow UI and trigger the DAG. Airflow will create a SparkApplication resource in Kubernetes. You can view the resource with kubectl and track the logs or status as needed.

In the Airflow UI, you’ll see the DAG execution flow, showing the start task, the Spark job, and the final task.

⸻

Conclusion

You’ve now seen how to run Spark jobs on Kubernetes using the Spark Operator and orchestrate them using Apache Airflow 3 point 0. This setup gives you the scalability of Kubernetes, the automation of Airflow, and the simplicity of declarative Spark applications.

⸻

Outro

Thanks for reading and watching. If you have any questions, feel free to reach out.

This is Young Gyu Kim. See you in the next video.