---
layout: documents
title: Apache Airflow - How to use SparkKubernetesOperator
author: Young Gyu Kim
email: credemol@gmail.com
breadcrumb:
  - name: Home
    url: /
  - name: Docs
    url: /documents/
  - name: BigData Foundry
    url: /documents/bigdata-foundry/

---
// docs/spark/spark-operator/index.adoc

= Spark Operator

:imagesdir: images

[.img-wide]
image::intro.png[]

== Introduction

In this document, we will explore the Spark Operator, a powerful tool for managing Apache Spark applications on Kubernetes. We will cover the installation process, configuration options, and how to create and manage Spark applications using the Spark Operator.

Additionally, we will discuss how to integrate the Spark Operator with Apache Airflow 3.0 using the *SparkKubernetesOperator* to orchestrate Spark jobs.

== What is Spark Operator?

*Apache Spark* is a powerful open-source distributed computing system that provides an easy-to-use interface for processing large datasets.

The *Spark Operator* automates the process of deploying Spark applications, managing their lifecycle, and monitoring their status. It provides a declarative way to define Spark applications using Kubernetes Custom Resource Definitions (CRDs), allowing users to easily create, update, and delete Spark applications using standard Kubernetes tools.





== Installation

=== Add the Spark Operator Helm repository

We are going to use 'spark-operator' provided by Kubeflow. First, check you have added the old version of spark-operator repository provided by Google. Then remove it.

Now, add the new Helm repository provided by Kubeflow.

.add new Helm repository
[source,shell]
----
$ helm repo add spark-operator https://kubeflow.github.io/spark-operator

$ helm repo update spark-operator

$ helm repo list | grep spark-operator

spark-operator          https://kubeflow.github.io/spark-operator
----

.pull the spark-operator chart
[source,shell]
----
$ helm pull spark-operator/spark-operator

# as of October 2025, the latest version is 2.3.0

----

.download the values.yaml file.
[source,shell]
----
$ helm show values --version 2.3.0 spark-operator/spark-operator > values-2.3.0.yaml
----

=== custom-values.yaml

You can customize the values.yaml file to fit your needs. Here is an example of custom-values.yaml.

.custom-values.yaml
[source,yaml]
----

spark:
  # -- List of namespaces where to run spark jobs.
  jobNamespaces:
  - spark-jobs
  - airflow
  - default
----

For spark.jobNamespace, you can specify the namespace where the Spark applications will be created.


=== Install the Spark Operator

[source,shell]
----
$ helm install spark-operator spark-operator/spark-operator --namespace spark-operator --create-namespace -f custom-values.yaml

----



== Uninstall the Spark Operator

[source,shell]
----
$ helm uninstall spark-operator -n spark-operator
----

== Create an example application

Spark-Operator examples can be found in the examples directory of the Spark-Operator repository.

* https://github.com/kubeflow/spark-operator/tree/master/examples

.examples/spark-pi.yaml
[source,yaml]
----
#
# Copyright 2017 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: spark-pi
  namespace: default
spec:
  type: Scala
  mode: cluster
  image: docker.io/library/spark:4.0.0
  imagePullPolicy: IfNotPresent
  mainClass: org.apache.spark.examples.SparkPi
  mainApplicationFile: local:///opt/spark/examples/jars/spark-examples.jar
  arguments:
  - "5000"
  sparkVersion: 4.0.0
  driver:
    labels:
      version: 4.0.0
    cores: 1
    memory: 512m
    serviceAccount: spark-operator-spark
    securityContext:
      capabilities:
        drop:
        - ALL
      runAsGroup: 185
      runAsUser: 185
      runAsNonRoot: true
      allowPrivilegeEscalation: false
      seccompProfile:
        type: RuntimeDefault
  executor:
    labels:
      version: 4.0.0
    instances: 1
    cores: 1
    memory: 512m
    securityContext:
      capabilities:
        drop:
        - ALL
      runAsGroup: 185
      runAsUser: 185
      runAsNonRoot: true
      allowPrivilegeEscalation: false
      seccompProfile:
        type: RuntimeDefault

----

I just changed the namespace to 'spark-jobs' in the example file.

[source,shell]
----
#$ kubectl get namespace spark-jobs || kubectl create namespace spark-jobs

# Create an example Spark application in the spark-jobs namespace
$ kubectl apply -f examples/spark-pi.yaml
----

=== Verify the Spark application

To verify the Spark application, you can check the logs of the driver pod.

[source,shell]
----
$ kubectl get pods
$ kubectl logs -f spark-pi-driver
----

== Upgrade the Spark Operator

[source,shell]
----
$ helm upgrade spark-operator spark-operator/spark-operator --namespace spark-operator -f custom-values.yaml
----

== Working with Airflow 3.0 and SparkKubernetesOperator

For more details about how to install Apache Airflow 3 on Kubernetes, please refer to the following document:

* https://nsalexamy.github.io/service-foundry/pages/documents/bigdata-foundry/airflow-with-service-foundry/[Installing Apache Airflow 3 on Kubernetes]
* https://youtu.be/JzIXVxYS0uQ[YouTube Video - Installing Apache Airflow 3 on Kubernetes]
* https://youtu.be/OS5t1Ubqp1k[YouTube Video - Installing Apache Airflow 3 on Kubernetes with GitOps using Service Foundry]



=== Extends Git Repository used in Airflow for DAGs

A directory named *spark-apps/* needs to be created under the *dags/* directory in the Airflow Git repository to store the Spark application YAML files.

.file structure in Airflow Git Repository
[source,shell]
----
$ tree dags --dirsfirst
dags
├── spark-apps
│   └── spark-pi.yaml
├── hello_world_dag.py
└── spark-py-example.py
----

File descriptions:

* spark-py-example.py: An example DAG that uses SparkKubernetesOperator to submit a Spark application.
* spark-apps/spark-pi.yaml: The Spark application YAML file used in the DAG.

[WARNING]
====
The application directory which is *spark-apps/* must be located under the *dags/* directory in the Airflow Git repository.
====

=== spark-py-example.py

This is an example DAG that uses SparkKubernetesOperator to submit a Spark application to the Kubernetes cluster.

[source,python]
----
from datetime import timedelta, datetime
from airflow.operators.python import PythonOperator
from airflow import DAG

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime.now() - timedelta(days=1),
    'email': ['airflow@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'max_active_runs': 1,
    'retries': 0,
}


def startBatch():
    print('##### startBatch #####')

def done():
    print('##### done #####')

with DAG(
    dag_id='spark_pi',
    start_date=datetime.now() - timedelta(days=1),
    default_args=default_args,
    schedule=None,
    tags=['example']
) as dag:
    spark_pi_task = SparkKubernetesOperator(
        task_id='spark_example',
        namespace='airflow',
        application_file='spark-apps/spark-pi.yaml',
        kubernetes_conn_id='kubernetes_default',

    )

    start_batch_task = PythonOperator(
        task_id='startBatch',
        python_callable=startBatch
    )
    done_task = PythonOperator(
        task_id='done',
        python_callable=done
    )


    start_batch_task >> spark_pi_task >> done_task
----

=== spark-apps/spark-pi.yaml

This is an example Spark application provided by Spark Operator. Make sure the namespace matches the one used in the Airflow DAG.

[source,yaml]
----
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: spark-pi
  # <1> Make sure the namespace matches the one used in the Airflow DAG
  namespace: airflow
spec:
  type: Scala
  mode: cluster
  image: docker.io/library/spark:4.0.0
  imagePullPolicy: IfNotPresent
  mainClass: org.apache.spark.examples.SparkPi
  mainApplicationFile: local:///opt/spark/examples/jars/spark-examples.jar
  arguments:
    - "5000"
  sparkVersion: 4.0.0
  driver:
    labels:
      version: 4.0.0
    cores: 1
    memory: 512m
    serviceAccount: spark-operator-spark
    securityContext:
      capabilities:
        drop:
          - ALL
      runAsGroup: 185
      runAsUser: 185
      runAsNonRoot: true
      allowPrivilegeEscalation: false
      seccompProfile:
        type: RuntimeDefault
  executor:
    labels:
      version: 4.0.0
    instances: 1
    cores: 1
    memory: 512m
    securityContext:
      capabilities:
        drop:
          - ALL
      runAsGroup: 185
      runAsUser: 185
      runAsNonRoot: true
      allowPrivilegeEscalation: false
      seccompProfile:
        type: RuntimeDefault
----

== RBAC for SparkKubernetesOperator

The airflow-worker service account needs to have permissions to create SparkApplication resources in the target namespace.

.spark-rbac.yaml
[source,yaml]
----
# spark-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: spark-application-role
  namespace: airflow
rules:
  - apiGroups: ["sparkoperator.k8s.io"]
    resources:
      - "sparkapplications"
      - "sparkapplications/status"
      - "sparkapplications/finalizers"
    verbs:
      - create
      - get
      - list
      - watch
      - update
      - patch
      - delete

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: spark-application-rolebinding
  namespace: airflow
subjects:
  - kind: ServiceAccount
    name: airflow-worker
    namespace: airflow
roleRef:
  kind: Role
  name: spark-application-role
  apiGroup: rbac.authorization.k8s.io
----

Apply the RBAC configuration:

[source,shell]
----
$ kubectl apply -f spark-rbac.yaml
----

== The result of executing the DAG

When you trigger the *spark_pi* DAG in Airflow, it will create a SparkApplication resource in the Kubernetes cluster. You can monitor the status of the Spark application using kubectl.

[source,shell]
----
$ kubectl get sparkapplications -n airflow
----

And you can see the graphical representation of the DAG in the Airflow web UI.

.Airflow DAG Execution
[.img-wide]
image::airflow-dag-execution.png[]

The second task is the SparkKubernetesOperator task that submits the Spark application to the Kubernetes cluster.

== Conclusion

This document provided an overview of how to install and use the Spark Operator on Kubernetes, as well as how to integrate it with Apache Airflow 3.0 using the SparkKubernetesOperator. By following the steps outlined in this document, you can easily deploy and manage Spark applications on your Kubernetes cluster using Airflow.

