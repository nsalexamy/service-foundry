Slide: Title – Running Spark Applications on Kubernetes with Spark Operator and Airflow 3.0
“Welcome to this guide. In this video, we’ll walk through how to run Spark applications on Kubernetes using the Spark Operator, and how to orchestrate them using Apache Airflow 3.0.”

⸻

Slide: Introduction
“We’ll cover installing the Spark Operator, deploying Spark applications as Kubernetes resources, and integrating those jobs into Airflow DAGs using the SparkKubernetesOperator.”

⸻

Slide: What is the Spark Operator?
“The Spark Operator simplifies Spark job management on Kubernetes. It lets you define Spark applications using custom resources, so they can be deployed and monitored just like native Kubernetes objects.”

⸻

Slide: Installing the Spark Operator with Helm
“First, we add the Helm repo provided by Kubeflow, then download the chart and a copy of the default values file. We edit that file to include the namespaces where Spark jobs will run, and install the operator using a custom values file.”

⸻

Slide: Uninstalling the Operator
“To uninstall the Spark Operator, we simply use Helm’s uninstall command targeting the spark-operator release.”

⸻

Slide: Create a Sample Spark Application
“We use the example spark-pi.yaml from the official GitHub repo. After optionally changing the namespace, we apply it with kubectl to submit a job to the cluster. You can check pod status and driver logs to verify the job ran successfully.”

⸻

Slide: Integrating with Airflow 3.0
“To integrate with Airflow, make sure Airflow 3 is already installed. You’ll organize your Git repository so that Spark applications are stored under the dags/spark-apps directory.”

⸻

Slide: File Structure and DAG Setup
“Your DAGs folder should include both the Spark YAML file and a Python DAG file. The DAG uses SparkKubernetesOperator to reference and submit the Spark application.”

⸻

Slide: Example DAG and Application
“The DAG defines a simple workflow with a start task, a Spark job task, and a done task. The YAML file for the Spark application must be in the same namespace defined in the DAG.”

⸻

Slide: RBAC for Airflow Worker
“Airflow needs permission to create SparkApplication resources. We apply a Role and RoleBinding to grant the Airflow worker service account the required access in the target namespace.”

⸻

Slide: Result of Triggering the DAG
“Once the DAG is triggered, a SparkApplication is created in Kubernetes. You can monitor its progress using kubectl and visualize the DAG execution in the Airflow UI.”

⸻

Slide: Conclusion
“That’s it. You’ve learned how to run Spark jobs on Kubernetes using the Spark Operator, and how to schedule them with Airflow 3. This setup combines Kubernetes scalability with Airflow’s orchestration power.”